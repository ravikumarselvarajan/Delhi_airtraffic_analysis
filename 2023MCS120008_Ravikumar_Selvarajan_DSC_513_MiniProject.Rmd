---
title: "Modelling AQI, weather and Delhi Air Traffic Delay "
author: "2023MCS120008_Ravikumar_Selvarajan"
date: "11/19/2023"
output: html_document
---
# Objectives:

# - To derive the parameters that impacts the precipitation behavior in terms of quantity and temporal behavior
# - To derive the behavioral pattern of AQI and to identify the top parameters that influences the AQI
# - To study the influence weather has on AQI and arrive at a model to predict the AQI given the other parameters   are known
# - To study the impact of weather and AQI on air traffic delays at Delhi airport
# - To deduce two or more models using which one can predict at the traffic delays if we know the AQI components/weather
# - To finalize one mode, and then train, test and evaluate the model's performance for traffic delay prediction
# - Via this we would also like to find out if the following hypothesis is true:
#   a. Precipitation impacts Traffic
#   b. High AQI during winter causes smog which will impact Traffic

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(lazyeval)
library(dplyr)
library(tidyr)
library(lubridate)
library(mosaic)
library(mosaicData)
install.packages("datasets/statisticalModeling_0.3.0.tar.gz", repos = NULL, type = "source")
library(statisticalModeling)
library(naniar)
library(visdat)
library(simputation)

Weather_Station <- read.csv("./datasets/Station_GeoLocation_Longitute_Latitude_Elevation_EPSG_4326.csv")
Weather_Bangalore <- read.csv("./datasets/Bangalore_1990_2022_BangaloreCity.csv")
Weather_Chennai <- read.csv("./datasets/Chennai_1990_2022_Madras.csv")                        
Weather_Delhi <- read.csv("./datasets/Delhi_NCR_1990_2022_Safdarjung.csv")
Weather_Lucknow <- read.csv("./datasets/Lucknow_1990_2022.csv")                           
Weather_Mumbai <- read.csv("./datasets/Mumbai_1990_2022_Santacruz.csv")
Weather_Jodhpur <- read.csv("./datasets/Rajasthan_1990_2022_Jodhpur.csv")                               
Weather_Bhubhneshwar <- read.csv("./datasets/weather_Bhubhneshwar_1990_2022.csv")
Weather_Rourkela <- read.csv("./datasets/weather_Rourkela_2021_2022.csv")
AQ_stations <- read.csv("./datasets/stations.csv")                       
AQ_station_hour <- read.csv("./datasets/station_hour.csv")                                              
AQ_station_day <- read.csv("./datasets/station_day.csv")                                               
AQ_city_day <- read.csv("./datasets/city_day.csv")
AQ_city_hour <- read.csv("./datasets/city_hour.csv")                                                 
Airport_delay <- read.csv("./Aiport_Delay.csv")
```

## Preliminary analysis:

## Under this section we will be performing cleaning of the data and exploratory data analysis.

## For every dataset, we are performing the following steps:
## - Observe the dimensions of the dataset
## - Have a glimpse of the dataset which will give a quick overview of the overall structure
## - Have a quick look at the top 6 and bottom 6 entires of the dataset
## - Checkout the column names to understand the fields that constitute the dataset
## - Study the structure of the dataset
## - Look at the Summary of dataset which will talk about each and every field with their charactersticks such as mean, max, min, NAs, class, etc.

## Then conduct detailed study of the missing parameters which includes the following: 
## - Get the total number of missing parameters
## - Study the Variable level missingness summary
## - Study the running span of the Missingness of key variables
## - Visualise missingness including their percentage
## - Visulaize the missing data via plot
## - Study the heat map of missingness of critical parameters
## - Visualize span of prcp missingness
## - Check for all types of missingness including "N/A", "NA", "na", " ","missing"

## Conduct study the relationship/dependencies of the missing data with other data in the dataset:
## - Create a shadow matrix of the dataset
## - Create nabular data by binding the shadow to the data
## - Bind the shadow only with missing columns and summarize the missing elements with the other parameters
## - Plot this relationship via ggplot and visualize the impact
## - Explore the missingness in various parameters, and display the missingness using `geom_miss_point'

## Perform imputations in two ways:
## - impute_all where all the missing data gets replaced with a value below the range by 10%. Ensure we can track the imputations.
##   visualize and study the impact of imputations over the dataset
## - impute via linear regression mechanism in relationship with other explanatory parameters visualize and study the impact of imputations over the dataset


##Analysing Station_GeoLocation_Longitute_Latitude_Elevation_EPSG_4326.csv
```{r}
## Have a look at the data

print ("The dimensions of the dataset")
dim(Weather_Station)

print("Lets have a glimpse of the dataset")
glimpse(Weather_Station)

print("Lets find the column names of the dataset")
names(Weather_Station)

print("Lets find the structure of the dataset")
str(Weather_Station)

print("Lets find the summary of the dataset")
summary(Weather_Station)


attach(Weather_Station)
## Only Elevation seems to have some missing data, lets zoom into them
Weather_Station[is.na(Elevation),]
miss_var_summary(Weather_Station)
prop_miss(Weather_Station)
## Nothing special about why Bubhneshwar and Rourkela alone seems to have elevation missing
## No cleaning needed of its data

```
##Analysing  and Performing Imputations on Bangalore_1990_2022_BangaloreCity.csv
```{r}
## Have a look at the data
print ("The dimensions of the dataset")
dim(Weather_Bangalore)

print("Lets have a glimpse of the dataset")
glimpse(Weather_Bangalore)

print("Lets see the head of the dataset")
head(Weather_Bangalore)

print("Lets see the tail of the dataset")
tail(Weather_Bangalore)

print("Lets find the column names of the dataset")
all_columns <- names(Weather_Bangalore)

print("Lets find the structure of the dataset")
str(Weather_Bangalore)

print("Lets find the summary of the dataset")
summary(Weather_Bangalore)

## Lets analyse the missing data of the dataset
n_miss(Weather_Bangalore) ## Total number of missing parameters
miss_var_summary(Weather_Bangalore) ## Missingness summary
miss_var_span(Weather_Bangalore, var = prcp, span_every = 250) ## Missingness span
miss_var_table(Weather_Bangalore)
vis_miss(Weather_Bangalore) ## visualise % of missing
gg_miss_upset(Weather_Bangalore) ## plot for missing data
gg_miss_fct(x = Weather_Bangalore, fct = prcp) ## Heat map of missingness
gg_miss_span(Weather_Bangalore, var = prcp, span_every = 250) ## Visualize span of prcp missingness

## With this we can clearly see the precipitation data is missing a lot (39%), followed by tmin
miss_scan_count(data = Weather_Bangalore, search = list("N/A", "NA", "na", " ","missing")) ## No empty cells

##Create shadow matrix data
head(as_shadow(Weather_Bangalore))

#Create nabular data by binding the shadow to the data
head(bind_shadow(Weather_Bangalore, only_miss = TRUE))

# Lets explore the relations ship with the missing values
Weather_Bangalore %>%
bind_shadow(only_miss = TRUE) %>%
group_by(prcp_NA) %>%
summarise(tavg_mean = mean(tavg),tavg_sd = sd(tavg))

# After adding NA, there the SD and mean has also become NA

bind_shadow(Weather_Bangalore) %>%
ggplot(aes(x = tavg,
color = prcp_NA)) +
geom_density() +
facet_wrap(~tmin_NA)

# Explore the missingness in precipitation and air temperature, and display the missingness using `geom_miss_point'
ggplot(Weather_Bangalore, aes(x = tavg,y = prcp)) + geom_miss_point()

ggplot(Weather_Bangalore, aes(x = tavg,y = prcp)) + geom_miss_point() +
facet_wrap(~year(dmy(time)))
# Looks like there are not too much of missing data

# We would like to impute all the missing data with value below the range by 10%
Weather_Bangalore_imp <- impute_below_all(Weather_Bangalore)
ggplot(Weather_Bangalore_imp, aes(x = tavg, y = prcp)) + geom_miss_point()

# But we need to track the imputed values as well
Weather_Bangalore_imp_track <- bind_shadow(Weather_Bangalore) %>% impute_below_all()
ggplot(Weather_Bangalore_imp_track, aes(x = prcp, fill = prcp_NA)) + geom_histogram()
ggplot(Weather_Bangalore_imp_track, aes(x = tmin, fill = tmin_NA)) + geom_histogram()

ggplot(Weather_Bangalore_imp_track, aes(x = tavg, y = prcp, color = prcp_NA)) + geom_point()

## So we have successfully imputed all the NA values here

# Now lets fix the important the critically missing parameters prcp and lm 
# via linear regression mechanism in relationship with other explanatory parameters
Weather_Bangalore_imp_lm_temp <- Weather_Bangalore %>% bind_shadow() %>% impute_lm(prcp ~ tavg + tmin) %>% impute_lm(tmin ~ tavg) %>% add_label_shadow()

ggplot(Weather_Bangalore_imp_lm_temp, aes(x = tavg, y = prcp, color = any_missing)) + geom_miss_point()

```


##Analysing and Performing Imputations on Chennai_1990_2022_Madras.csv
```{r}
## Have a look at the data

print ("The dimensions of the dataset")
dim(Weather_Chennai)

print("Lets have a glimpse of the dataset")
glimpse(Weather_Chennai)

print("Lets see the head of the dataset")
head(Weather_Chennai)

print("Lets see the tail of the dataset")
tail(Weather_Chennai)

print("Lets find the column names of the dataset")
all_columns <- names(Weather_Chennai)

print("Lets find the structure of the dataset")
str(Weather_Chennai)

print("Lets find the summary of the dataset")
summary(Weather_Chennai)

sum(is.na(Weather_Chennai))
## About 9016 entries are NA

## Lets analyse the missing data of the dataset
n_miss(Weather_Chennai) ## Total number of missing parameters
miss_var_summary(Weather_Chennai) ## Missingness summary
miss_var_span(Weather_Chennai, var = prcp, span_every = 250) ## Missingness span
miss_var_table(Weather_Chennai)
vis_miss(Weather_Chennai) ## visualise % of missing
gg_miss_upset(Weather_Chennai) ## plot for missing data
gg_miss_fct(x = Weather_Chennai, fct = prcp) ## Heat map of missingness
gg_miss_span(Weather_Chennai, var = prcp, span_every = 250) ## Visualize span of prcp missingness

## With this we can clearly see the precipitation data is missing a lot (41%), followed by tmin
miss_scan_count(data = Weather_Chennai, search = list("N/A", "NA", "na", " ","missing")) ## No empty cells

##Create shadow matrix data
head(as_shadow(Weather_Chennai))

#Create nabular data by binding the shadow to the data
head(bind_shadow(Weather_Chennai, only_miss = TRUE))

# Lets explore the relations ship with the missing values
Weather_Chennai %>%
bind_shadow(only_miss = TRUE) %>%
group_by(prcp_NA) %>%
summarise(tavg_mean = mean(tavg),tavg_sd = sd(tavg))

# After adding NA, there the SD and mean has also become NA

bind_shadow(Weather_Chennai) %>%
ggplot(aes(x = tavg,
color = prcp_NA)) +
geom_density() +
facet_wrap(~tmin_NA)

# Explore the missingness in precipitation and air temperature, and display the missingness using `geom_miss_point'
ggplot(Weather_Chennai, aes(x = tavg,y = prcp)) + geom_miss_point()

ggplot(Weather_Chennai, aes(x = tavg,y = prcp)) + geom_miss_point() +
facet_wrap(~year(dmy(time)))
# Looks like there are not too much of missing data

# We would like to impute all the missing data with value below the range by 10%
Weather_Chennai_imp <- impute_below_all(Weather_Chennai)
ggplot(Weather_Chennai_imp, aes(x = tavg, y = prcp)) + geom_miss_point()

# But we need to track the imputed values as well
Weather_Chennai_imp_track <- bind_shadow(Weather_Chennai) %>% impute_below_all()
ggplot(Weather_Chennai_imp_track, aes(x = prcp, fill = prcp_NA)) + geom_histogram()
ggplot(Weather_Chennai_imp_track, aes(x = tmin, fill = tmin_NA)) + geom_histogram()

ggplot(Weather_Chennai_imp_track, aes(x = tavg, y = prcp, color = prcp_NA)) + geom_point()

## So we can successfully imputed all the NA values here

# Now lets fix the important the critically missing parameters prcp and lm 
# via linear regression mechanism in relationship with other explanatory parameters
Weather_Chennai_imp_lm_temp <- Weather_Chennai %>% bind_shadow() %>% impute_lm(prcp ~ tavg + tmin) %>% impute_lm(tmin ~ tavg) %>% add_label_shadow()

ggplot(Weather_Chennai_imp_lm_temp, aes(x = tavg, y = prcp, color = any_missing)) + geom_miss_point()

```

##Analysing and Performing Imputations on Delhi_NCR_1990_2022_Safdarjung.csv
```{r}
## Have a look at the data

print ("The dimensions of the dataset")
dim(Weather_Delhi)

print("Lets have a glimpse of the dataset")
glimpse(Weather_Delhi)

print("Lets see the head of the dataset")
head(Weather_Delhi)

print("Lets see the tail of the dataset")
tail(Weather_Delhi)

print("Lets find the column names of the dataset")
all_columns <- names(Weather_Delhi)

print("Lets find the structure of the dataset")
str(Weather_Delhi)

print("Lets find the summary of the dataset")
summary(Weather_Delhi)

sum(is.na(Weather_Delhi))
## About 8303 entries are NA
## Lets analyse the missing data of the dataset
n_miss(Weather_Delhi) ## Total number of missing parameters
miss_var_summary(Weather_Delhi) ## Missingness summary
miss_var_span(Weather_Delhi, var = prcp, span_every = 250) ## Missingness spread
miss_var_table(Weather_Delhi)
vis_miss(Weather_Delhi) ## visualise % of missing
gg_miss_upset(Weather_Delhi) ## plot for missing data
gg_miss_fct(x = Weather_Delhi, fct = prcp) ## Heat map of missingness
gg_miss_span(Weather_Delhi, var = prcp, span_every = 250) ## Visualize span of prcp missingness

## With this we can clearly see the precipitation data is missing a lot (51%), followed by tmin
miss_scan_count(data = Weather_Delhi, search = list("N/A", "NA", "na", " ","missing")) ## No empty cells

##Create shadow matrix data
head(as_shadow(Weather_Delhi))

#Create nabular data by binding the shadow to the data
head(bind_shadow(Weather_Delhi, only_miss = TRUE))

# Lets explore the relations ship with the missing values
Weather_Delhi %>%
bind_shadow(only_miss = TRUE) %>%
group_by(prcp_NA) %>%
summarise(tavg_mean = mean(tavg),tavg_sd = sd(tavg))

# After adding NA, there the SD and mean has also become NA

bind_shadow(Weather_Delhi) %>%
ggplot(aes(x = tavg,
color = prcp_NA)) +
geom_density() +
facet_wrap(~tmin_NA)

# Explore the missingness in precipitation and air temperature, and display the missingness using `geom_miss_point'
ggplot(Weather_Delhi, aes(x = tavg,y = prcp)) + geom_miss_point()

ggplot(Weather_Delhi, aes(x = tavg,y = prcp)) + geom_miss_point() +
facet_wrap(~year(dmy(time)))
# Looks like there are not too much of missing data

# We would like to impute all the missing data with value below the range by 10%
Weather_Delhi_imp <- impute_below_all(Weather_Delhi)
ggplot(Weather_Delhi_imp, aes(x = tavg, y = prcp)) + geom_miss_point()

# But we need to track the imputed values as well
Weather_Delhi_imp_track <- bind_shadow(Weather_Delhi) %>% impute_below_all()
ggplot(Weather_Delhi_imp_track, aes(x = prcp, fill = prcp_NA)) + geom_histogram()
ggplot(Weather_Delhi_imp_track, aes(x = tmin, fill = tmin_NA)) + geom_histogram()

ggplot(Weather_Delhi_imp_track, aes(x = tavg, y = prcp, color = prcp_NA)) + geom_point()

## So we can successfully imputed all the NA values here

# Now lets fix the important the critically missing parameters prcp and lm 
# via linear regression mechanism in relationship with other explanatory parameters
Weather_Delhi_imp_lm_temp <- Weather_Delhi %>% bind_shadow() %>% impute_lm(prcp ~ tavg + tmin) %>% impute_lm(tmin ~ tavg) %>% add_label_shadow()

ggplot(Weather_Delhi_imp_lm_temp, aes(x = tavg, y = prcp, color = any_missing)) + geom_miss_point()


```

##Analysing and Performing Imputations on Lucknow_1990_2022.csv
```{r}
## Have a look at the data

print ("The dimensions of the dataset")
dim(Weather_Lucknow)

print("Lets have a glimpse of the dataset")
glimpse(Weather_Lucknow)

print("Lets see the head of the dataset")
head(Weather_Lucknow)

print("Lets see the tail of the dataset")
tail(Weather_Lucknow)

print("Lets find the column names of the dataset")
all_columns <- names(Weather_Lucknow)

print("Lets find the structure of the dataset")
str(Weather_Lucknow)

print("Lets find the summary of the dataset")
summary(Weather_Lucknow)

sum(is.na(Weather_Lucknow))
## About 11358 entries are NA

## Lets analyse the missing data of the dataset
n_miss(Weather_Lucknow) ## Total number of missing parameters
miss_var_summary(Weather_Lucknow) ## Missingness summary
miss_var_span(Weather_Lucknow, var = prcp, span_every = 250) ## Missingness spread
miss_var_table(Weather_Lucknow)
vis_miss(Weather_Lucknow) ## visualise % of missing
gg_miss_upset(Weather_Lucknow) ## plot for missing data
gg_miss_fct(x = Weather_Lucknow, fct = prcp) ## Heat map of missingness
gg_miss_span(Weather_Lucknow, var = prcp, span_every = 250) ## Visualize span of prcp missingness

## With this we can clearly see the precipitation data is missing a lot (39%), followed by tmin
miss_scan_count(data = Weather_Lucknow, search = list("N/A", "NA", "na", " ","missing")) ## No empty cells

##Create shadow matrix data
head(as_shadow(Weather_Lucknow))

#Create nabular data by binding the shadow to the data
head(bind_shadow(Weather_Lucknow, only_miss = TRUE))

# Lets explore the relations ship with the missing values
Weather_Lucknow %>%
bind_shadow(only_miss = TRUE) %>%
group_by(prcp_NA) %>%
summarise(tavg_mean = mean(tavg),tavg_sd = sd(tavg))

# After adding NA, there the SD and mean has also become NA

bind_shadow(Weather_Lucknow) %>%
ggplot(aes(x = tavg,
color = prcp_NA)) +
geom_density() +
facet_wrap(~tmin_NA)

# Explore the missingness in precipitation and air temperature, and display the missingness using `geom_miss_point'
ggplot(Weather_Lucknow, aes(x = tavg,y = prcp)) + geom_miss_point()

ggplot(Weather_Lucknow, aes(x = tavg,y = prcp)) + geom_miss_point() +
facet_wrap(~year(dmy(time)))
# Looks like there are not too much of missing data

# We would like to impute all the missing data with value below the range by 10%
Weather_Lucknow_imp <- impute_below_all(Weather_Lucknow)
ggplot(Weather_Lucknow_imp, aes(x = tavg, y = prcp)) + geom_miss_point()

# But we need to track the imputed values as well
Weather_Lucknow_imp_track <- bind_shadow(Weather_Lucknow) %>% impute_below_all()
ggplot(Weather_Lucknow_imp_track, aes(x = prcp, fill = prcp_NA)) + geom_histogram()
ggplot(Weather_Lucknow_imp_track, aes(x = tmin, fill = tmin_NA)) + geom_histogram()

ggplot(Weather_Lucknow_imp_track, aes(x = tavg, y = prcp, color = prcp_NA)) + geom_point()

## So we can successfully imputed all the NA values here

# Now lets fix the important the critically missing parameters prcp and lm 
# via linear regression mechanism in relationship with other explanatory parameters
Weather_Lucknow_imp_lm_temp <- Weather_Lucknow %>% bind_shadow() %>% impute_lm(prcp ~ tavg + tmin) %>% impute_lm(tmin ~ tavg) %>% add_label_shadow()

ggplot(Weather_Lucknow_imp_lm_temp, aes(x = tavg, y = prcp, color = any_missing)) + geom_miss_point()

```

##Analysing and Performing Imputations on Mumbai_1990_2022_Santacruz.csv
```{r}
## Have a look at the data

print ("The dimensions of the dataset")
dim(Weather_Mumbai)

print("Lets have a glimpse of the dataset")
glimpse(Weather_Mumbai)

print("Lets see the head of the dataset")
head(Weather_Mumbai)

print("Lets see the tail of the dataset")
tail(Weather_Mumbai)

print("Lets find the column names of the dataset")
all_columns <- names(Weather_Mumbai)

print("Lets find the structure of the dataset")
str(Weather_Mumbai)

print("Lets find the summary of the dataset")
summary(Weather_Mumbai)

sum(is.na(Weather_Mumbai))
## About 9053 entries are NA

## Lets analyse the missing data of the dataset
n_miss(Weather_Mumbai) ## Total number of missing parameters
miss_var_summary(Weather_Mumbai) ## Missingness summary
miss_var_span(Weather_Mumbai, var = prcp, span_every = 250) ## Missingness spread
miss_var_table(Weather_Mumbai)
vis_miss(Weather_Mumbai) ## visualise % of missing
gg_miss_upset(Weather_Mumbai) ## plot for missing data
gg_miss_fct(x = Weather_Mumbai, fct = prcp) ## Heat map of missingness
gg_miss_span(Weather_Mumbai, var = prcp, span_every = 250) ## Visualize span of prcp missingness

## With this we can clearly see the precipitation data is missing a lot (39%), followed by tmin
miss_scan_count(data = Weather_Mumbai, search = list("N/A", "NA", "na", " ","missing")) ## No empty cells

##Create shadow matrix data
head(as_shadow(Weather_Mumbai))

#Create nabular data by binding the shadow to the data
head(bind_shadow(Weather_Mumbai, only_miss = TRUE))

# Lets explore the relations ship with the missing values
Weather_Mumbai %>%
bind_shadow(only_miss = TRUE) %>%
group_by(prcp_NA) %>%
summarise(tavg_mean = mean(tavg),tavg_sd = sd(tavg))

# After adding NA, there the SD and mean has also become NA

bind_shadow(Weather_Mumbai) %>%
ggplot(aes(x = tavg,
color = prcp_NA)) +
geom_density() +
facet_wrap(~tmin_NA)

# Explore the missingness in precipitation and air temperature, and display the missingness using `geom_miss_point'
ggplot(Weather_Mumbai, aes(x = tavg,y = prcp)) + geom_miss_point()

ggplot(Weather_Mumbai, aes(x = tavg,y = prcp)) + geom_miss_point() +
facet_wrap(~year(dmy(time)))
# Looks like there are not too much of missing data

# We would like to impute all the missing data with value below the range by 10%
Weather_Mumbai_imp <- impute_below_all(Weather_Mumbai)
ggplot(Weather_Mumbai_imp, aes(x = tavg, y = prcp)) + geom_miss_point()

# But we need to track the imputed values as well
Weather_Mumbai_imp_track <- bind_shadow(Weather_Mumbai) %>% impute_below_all()
ggplot(Weather_Mumbai_imp_track, aes(x = prcp, fill = prcp_NA)) + geom_histogram()
ggplot(Weather_Mumbai_imp_track, aes(x = tmin, fill = tmin_NA)) + geom_histogram()

ggplot(Weather_Mumbai_imp_track, aes(x = tavg, y = prcp, color = prcp_NA)) + geom_point()

## So we can successfully imputed all the NA values here

# Now lets fix the important the critically missing parameters prcp and lm 
# via linear regression mechanism in relationship with other explanatory parameters
Weather_Mumbai_imp_lm_temp <- Weather_Mumbai %>% bind_shadow() %>% impute_lm(prcp ~ tavg + tmin) %>% impute_lm(tmin ~ tavg) %>% add_label_shadow()

ggplot(Weather_Mumbai_imp_lm_temp, aes(x = tavg, y = prcp, color = any_missing)) + geom_miss_point()

```

##Analysing and Performing Imputations on Rajasthan_1990_2022_Jodhpur.csv
```{r}
## Have a look at the data


print ("The dimensions of the dataset")
dim(Weather_Jodhpur)

print("Lets have a glimpse of the dataset")
glimpse(Weather_Jodhpur)

print("Lets see the head of the dataset")
head(Weather_Jodhpur)

print("Lets see the tail of the dataset")
tail(Weather_Jodhpur)

print("Lets find the column names of the dataset")
all_columns <- names(Weather_Jodhpur)

print("Lets find the structure of the dataset")
str(Weather_Jodhpur)

print("Lets find the summary of the dataset")
summary(Weather_Jodhpur)

sum(is.na(Weather_Jodhpur))
## About 6708 entries are NA

## Lets analyse the missing data of the dataset
n_miss(Weather_Jodhpur) ## Total number of missing parameters
miss_var_summary(Weather_Jodhpur) ## Missingness summary
miss_var_span(Weather_Jodhpur, var = prcp, span_every = 250) ## Missingness spread
miss_var_table(Weather_Jodhpur)
vis_miss(Weather_Jodhpur) ## visualise % of missing
gg_miss_upset(Weather_Jodhpur) ## plot for missing data
gg_miss_fct(x = Weather_Jodhpur, fct = prcp) ## Heat map of missingness
gg_miss_span(Weather_Jodhpur, var = prcp, span_every = 250) ## Visualize span of prcp missingness

## With this we can clearly see the precipitation data is missing a lot (39%), followed by tmin
miss_scan_count(data = Weather_Jodhpur, search = list("N/A", "NA", "na", " ","missing")) ## No empty cells

##Create shadow matrix data
head(as_shadow(Weather_Jodhpur))

#Create nabular data by binding the shadow to the data
head(bind_shadow(Weather_Jodhpur, only_miss = TRUE))

# Lets explore the relations ship with the missing values
Weather_Jodhpur %>%
bind_shadow(only_miss = TRUE) %>%
group_by(prcp_NA) %>%
summarise(tavg_mean = mean(tavg),tavg_sd = sd(tavg))

# After adding NA, there the SD and mean has also become NA

bind_shadow(Weather_Jodhpur) %>%
ggplot(aes(x = tavg,
color = prcp_NA)) +
geom_density() +
facet_wrap(~tmin_NA)

# Explore the missingness in precipitation and air temperature, and display the missingness using `geom_miss_point'
ggplot(Weather_Jodhpur, aes(x = tavg,y = prcp)) + geom_miss_point()

ggplot(Weather_Jodhpur, aes(x = tavg,y = prcp)) + geom_miss_point() +
facet_wrap(~year(dmy(time)))
# Looks like there are not too much of missing data

# We would like to impute all the missing data with value below the range by 10%
Weather_Jodhpur_imp <- impute_below_all(Weather_Jodhpur)
ggplot(Weather_Jodhpur_imp, aes(x = tavg, y = prcp)) + geom_miss_point()

# But we need to track the imputed values as well
Weather_Jodhpur_imp_track <- bind_shadow(Weather_Jodhpur) %>% impute_below_all()
ggplot(Weather_Jodhpur_imp_track, aes(x = prcp, fill = prcp_NA)) + geom_histogram()
ggplot(Weather_Jodhpur_imp_track, aes(x = tmin, fill = tmin_NA)) + geom_histogram()

ggplot(Weather_Jodhpur_imp_track, aes(x = tavg, y = prcp, color = prcp_NA)) + geom_point()

## So we can successfully imputed all the NA values here

# Now lets fix the important the critically missing parameters prcp and lm 
# via linear regression mechanism in relationship with other explanatory parameters
Weather_Jodhpur_imp_lm_temp <- Weather_Jodhpur %>% bind_shadow() %>% impute_lm(prcp ~ tavg + tmin) %>% impute_lm(tmin ~ tavg) %>% add_label_shadow()

ggplot(Weather_Jodhpur_imp_lm_temp, aes(x = tavg, y = prcp, color = any_missing)) + geom_miss_point()

```

##Analysing weather_Bhubhneshwar_1990_2022.csv")
```{r}
## Have a look at the data

#definitely has more columns than the cities that we have seen so far

print ("The dimensions of the dataset")
dim(Weather_Bhubhneshwar)
#OK, so we have 11 columns, 6 more than others

print("Lets have a glimpse of the dataset")
glimpse(Weather_Bhubhneshwar)

print("Lets see the head of the dataset")
head(Weather_Bhubhneshwar)

print("Lets see the tail of the dataset")
tail(Weather_Bhubhneshwar)

print("Lets find the column names of the dataset")
names(Weather_Bhubhneshwar)

## So the additional columns are: snow, wind direction, wind speed, wind pgt, pressure and tsunami

print("Lets find the structure of the dataset")
str(Weather_Bhubhneshwar)

print("Lets find the summary of the dataset")
summary(Weather_Bhubhneshwar)

sum(is.na(Weather_Bhubhneshwar))
## About 75100 entries are NA
## Lets analyse the missing data of the dataset
n_miss(Weather_Bhubhneshwar) ## Total number of missing parameters
miss_var_summary(Weather_Bhubhneshwar) ## Missingness summary
miss_var_span(Weather_Bhubhneshwar, var = prcp, span_every = 250) ## Missingness spread
miss_var_table(Weather_Bhubhneshwar)
vis_miss(Weather_Bhubhneshwar) ## visualise % of missing
gg_miss_upset(Weather_Bhubhneshwar) ## plot for missing data
gg_miss_fct(x = Weather_Bhubhneshwar, fct = prcp) ## Heat map of missingness
gg_miss_span(Weather_Bhubhneshwar, var = prcp, span_every = 250) ## Visualize span of prcp missingness

## With this we can clearly see the precipitation data is missing a lot (42.7%)
## Actually there are other datasets which have huge missing data like snow - but these are almost meaningless 
## datasets and hence we are not bothered too much about them
miss_scan_count(data = Weather_Bhubhneshwar, search = list("N/A", "NA", "na", " ","missing")) ## No empty cells

##Create shadow matrix data
head(as_shadow(Weather_Bhubhneshwar))

#Create nabular data by binding the shadow to the data
head(bind_shadow(Weather_Bhubhneshwar, only_miss = TRUE))

# Lets explore the relations ship with the missing values
Weather_Bhubhneshwar %>%
bind_shadow(only_miss = TRUE) %>%
group_by(prcp_NA) %>%
summarise(tavg_mean = mean(tavg),tavg_sd = sd(tavg))

# After adding NA, there the SD and mean has also become NA

bind_shadow(Weather_Bhubhneshwar) %>%
ggplot(aes(x = tavg,
color = prcp_NA)) +
geom_density() +
facet_wrap(~tmin_NA)

# Explore the missingness in precipitation and air temperature, and display the missingness using `geom_miss_point'
ggplot(Weather_Bhubhneshwar, aes(x = tavg,y = prcp)) + geom_miss_point()

ggplot(Weather_Bhubhneshwar, aes(x = tavg,y = prcp)) + geom_miss_point() +
facet_wrap(~year(dmy(time)))
# Looks like there are not too much of missing data

```

##Analysing weather_Rourkela_2021_2022.csv")
```{r}
## Have a look at the data
#definitely has more columns than the cities that we have seen so far

print ("The dimensions of the dataset")
dim(Weather_Rourkela)
#OK, so we have 11 columns, 6 more than others

print("Lets have a glimpse of the dataset")
glimpse(Weather_Rourkela)

print("Lets see the head of the dataset")
head(Weather_Rourkela)

print("Lets see the tail of the dataset")
tail(Weather_Rourkela)

print("Lets find the column names of the dataset")
names(Weather_Rourkela)

## So the additional columns are: snow, wind direction, wind speed, wind pgt, pressure and tsunami

print("Lets find the structure of the dataset")
str(Weather_Rourkela)

print("Lets find the summary of the dataset")
summary(Weather_Rourkela)

sum(is.na(Weather_Rourkela))
## About 1293 entries are NA

## Lets analyse the missing data of the dataset
n_miss(Weather_Rourkela) ## Total number of missing parameters
miss_var_summary(Weather_Rourkela) ## Missingness summary
miss_var_span(Weather_Rourkela, var = prcp, span_every = 250) ## Missingness spread
miss_var_table(Weather_Rourkela)
vis_miss(Weather_Rourkela) ## visualise % of missing
gg_miss_upset(Weather_Rourkela) ## plot for missing data
gg_miss_fct(x = Weather_Rourkela, fct = prcp) ## Heat map of missingness
gg_miss_span(Weather_Rourkela, var = prcp, span_every = 250) ## Visualize span of prcp missingness

## With this we can clearly see the precipitation data is missing a lot (70%)
## Actually there are other datasets which have huge missing data like snow - but these are almost meaningless 
## datasets and hence we are not bothered too much about them
Weather_Rourkela
miss_scan_count(data = Weather_Rourkela, search = list("N/A", "NA", "na", " ","missing")) ## No empty cells

##Create shadow matrix data
as_shadow(Weather_Rourkela)

#Create nabular data by binding the shadow to the data
bind_shadow(Weather_Rourkela, only_miss = TRUE)

# Lets explore the relations ship with the missing values
Weather_Rourkela %>%
bind_shadow(only_miss = TRUE) %>%
group_by(prcp_NA) %>%
summarise(tavg_mean = mean(tavg),tavg_sd = sd(tavg))

# After adding NA, there the SD and mean has also become NA

bind_shadow(Weather_Rourkela) %>%
ggplot(aes(x = tavg,
color = prcp_NA)) +
geom_density() +
facet_wrap(~tmin_NA)

# Explore the missingness in precipitation and air temperature, and display the missingness using `geom_miss_point'
ggplot(Weather_Rourkela, aes(x = tavg,y = prcp)) + geom_miss_point()

ggplot(Weather_Rourkela, aes(x = tavg,y = prcp)) + geom_miss_point() +
facet_wrap(~year(dmy(time)))
# Looks like there are not too much of missing data

```

## With the analysis of the weather data so far, we have seen that the precipitation is the most missing parameter and it followed Missing At Random (MAR pattern) - as they are missing consistently by more than 35% and will have severe impact on the data quality if not imputed. So in all of them we have cleaned them up by doing both impute_all and also by using linear regression method - and both the results are sasitfactory which we have verified using various plots. Now we will go ahead and look at the AQI datasets.


##Analysing AQI stations: stations.csv
```{r}
## Have a look at the data

print ("The dimensions of the dataset")
dim(AQ_stations)

print("Lets have a glimpse of the dataset")
glimpse(AQ_stations)

print("Lets find the column names of the dataset")
names(AQ_stations)

print("Lets find the structure of the dataset")
str(AQ_stations)

print("Lets find the summary of the dataset")
summary(AQ_stations)


attach(AQ_stations)

AQ_stations [AQ_stations == ""] <- NA
## There is no records with NA but there are records with missing data.
## Lets fill them with NA and then find it.
AQ_stations[is.na(Status),]
```
##Analysing and Performing Imputations on AQI Station Hour wise - station_hour.csv
```{r}
## Have a look at the data

print ("The dimensions of the dataset")
dim(AQ_station_hour)

print("Lets have a glimpse of the dataset")
glimpse(AQ_station_hour)

print("Lets find the column names of the dataset")
names(AQ_station_hour)

print("Lets find the structure of the dataset")
str(AQ_station_hour)

print("Lets find the summary of the dataset")
summary(AQ_station_hour)

attach(AQ_station_hour)

AQ_station_hour [AQ_station_hour == ""] <- NA

## So the air quality seems to be dependent on 12 parameters
## There are too many NAs/missing data amongst them:
## PM2.5:647689  PM10:1119252 NO:553711       NO2:528973      NOx:490808  CO:1236618 
## SO2:499302    O3:742737    Benzene:725973  Toluene:861579  Xylene:1042366

AQ_station_hour %>% group_by(AQI_Bucket)%>%count()

## Lets analyse the missing data of the dataset
n_miss(AQ_station_hour) ## Total number of missing parameters
miss_var_summary(AQ_station_hour) ## Missingness summary
miss_var_span(AQ_station_hour, var = AQI, span_every = 250) ## Missingness spread
miss_var_table(AQ_station_hour)
## vis_miss(AQ_station_hour) Unable to visualise % of missing due to large data size
gg_miss_upset(AQ_station_hour) ## plot for missing data
gg_miss_fct(x = AQ_station_hour, fct = AQI) ## Heat map of missingness
gg_miss_span(AQ_station_hour, var = AQI, span_every = 250) ## Visualize span of AQI missingness

## With this we can clearly see the AQI data is missing a lot (22%)
## There are other dataset missing heavily as well but for us AQI is the single most critical dataset
miss_scan_count(data = AQ_station_hour, search = list("N/A", "NA", "na", " ","missing")) ## No empty cells

##Create shadow matrix data
head(as_shadow(AQ_station_hour))

#Create nabular data by binding the shadow to the data
head(bind_shadow(AQ_station_hour, only_miss = TRUE))

# Lets explore the relations ship with the other key missing values
AQ_station_hour %>%
bind_shadow(only_miss = TRUE) %>%
group_by(AQI_NA) %>%
summarise(tCO_mean = mean(CO),CO_sd = sd(CO))

# After adding NA, there the SD and mean has also become NA

bind_shadow(AQ_station_hour) %>%
ggplot(aes(x = CO,
color = AQI_NA)) +
geom_density() +
facet_wrap(~O3_NA)

# Explore the missingness in precipitation and air temperature, and display the missingness using `geom_miss_point'
ggplot(AQ_station_hour, aes(x = CO,y = AQI)) + geom_miss_point()

# Looks like there are not too much of missing data

# We would like to impute all the missing data with value below the range by 10%
AQ_station_hour_imp <- impute_below_all(AQ_station_hour)
ggplot(AQ_station_hour_imp, aes(x = CO, y = AQI)) + geom_miss_point()

# But we need to track the imputed values as well
AQ_station_hour_imp_track <- bind_shadow(AQ_station_hour) %>% impute_below_all()
ggplot(AQ_station_hour_imp_track, aes(x = AQI, fill = AQI_NA)) + geom_histogram()
ggplot(AQ_station_hour_imp_track, aes(x = O3, fill = O3_NA)) + geom_histogram()

ggplot(AQ_station_hour_imp_track, aes(x = CO, y = AQI, color = AQI_NA)) + geom_point()

## So we can successfully imputed all the NA values here

# Now lets fix the important the critically missing parameters prcp and lm 
# via linear regression mechanism in relationship with other explanatory parameters
AQ_station_hour_imp_lm_temp <- AQ_station_hour %>% bind_shadow() %>% impute_lm(AQI ~ CO + O3) %>% impute_lm(O3 ~ CO) %>% add_label_shadow()

ggplot(AQ_station_hour_imp_lm_temp, aes(x = CO, y = AQI, color = any_missing)) + geom_miss_point()

```

##Analysing and Performing Imputations on AQ_station_day  - station_day.csv
```{r}
## Have a look at the data

print ("The dimensions of the dataset")
dim(AQ_station_day)

print("Lets have a glimpse of the dataset")
glimpse(AQ_station_day)

print("Lets find the column names of the dataset")
names(AQ_station_day)

print("Lets find the structure of the dataset")
str(AQ_station_day)

print("Lets find the summary of the dataset")
summary(AQ_station_day)

attach(AQ_station_day)

AQ_station_day [AQ_station_day == ""] <- NA

## So the air quality seems to be dependent on 12 parameters
## There are too many NAs/missing data amongst them:
## PM2.5: 21625 PM10: 42706 NO: 17106 NO2: 16547 NOx: 15500 NH3: 48105 
## CO:  12998  SO2: 25204  O3: 25568 Benzene: 31455 Toluene: 38702 Xylene: 85137
AQ_station_day %>% group_by(AQI_Bucket)%>%count()
## Looks like Moderate entries are the highest ones, followed by Satisfactory 
## but third highest is NA entries...

AQ_station_day %>% group_by(AQI_Bucket)%>%count()

## Lets analyse the missing data of the dataset
n_miss(AQ_station_day) ## Total number of missing parameters
miss_var_summary(AQ_station_day) ## Missingness summary
miss_var_span(AQ_station_day, var = AQI, span_every = 250) ## Missingness spread
miss_var_table(AQ_station_day)
## vis_miss(AQ_station_day) Unable to visualise % of missing due to large data size
gg_miss_upset(AQ_station_day) ## plot for missing data
gg_miss_fct(x = AQ_station_day, fct = AQI) ## Heat map of missingness
gg_miss_span(AQ_station_day, var = AQI, span_every = 250) ## Visualize span of AQI missingness

## With this we can clearly see the AQI data is missing a lot (19%)
## There are other dataset missing heavily as well but for us AQI is the single most critical dataset
miss_scan_count(data = AQ_station_day, search = list("N/A", "NA", "na", " ","missing")) ## No empty cells

##Create shadow matrix data
head(as_shadow(AQ_station_day))

#Create nabular data by binding the shadow to the data
head(bind_shadow(AQ_station_day, only_miss = TRUE))

# Lets explore the relations ship with the missing values
AQ_station_day %>%
bind_shadow(only_miss = TRUE) %>%
group_by(AQI_NA) %>%
summarise(tCO_mean = mean(CO),CO_sd = sd(CO))

# After adding NA, there the SD and mean has also become NA

bind_shadow(AQ_station_day) %>%
ggplot(aes(x = CO,
color = AQI_NA)) +
geom_density() +
facet_wrap(~O3_NA)

# Explore the missingness in precipitation and air temperature, and display the missingness using `geom_miss_point'
ggplot(AQ_station_day, aes(x = CO,y = AQI)) + geom_miss_point()

# Looks like there are not too much of missing data

# We would like to impute all the missing data with value below the range by 10%
AQ_station_day_imp <- impute_below_all(AQ_station_day)
ggplot(AQ_station_day_imp, aes(x = CO, y = AQI)) + geom_miss_point()

# But we need to track the imputed values as well
AQ_station_day_imp_track <- bind_shadow(AQ_station_day) %>% impute_below_all()
ggplot(AQ_station_day_imp_track, aes(x = AQI, fill = AQI_NA)) + geom_histogram()
ggplot(AQ_station_day_imp_track, aes(x = O3, fill = O3_NA)) + geom_histogram()

ggplot(AQ_station_day_imp_track, aes(x = CO, y = AQI, color = AQI_NA)) + geom_point()

## So we can successfully imputed all the NA values here

# Now lets fix the important the critically missing parameters prcp and lm 
# via linear regression mechanism in relationship with other explanatory parameters
AQ_station_day_imp_lm_temp <- AQ_station_day %>% bind_shadow() %>% impute_lm(AQI ~ CO + O3) %>% impute_lm(O3 ~ CO) %>% add_label_shadow()

ggplot(AQ_station_day_imp_lm_temp, aes(x = CO, y = AQI, color = any_missing)) + geom_miss_point()
```
##Analysing and Performing Imputations on AQ_city_day <- read.csv("./datasets/city_day.csv")
```{r}
## Have a look at the data

print ("The dimensions of the dataset")
dim(AQ_city_day)

print("Lets have a glimpse of the dataset")
glimpse(AQ_city_day)

print("Lets find the column names of the dataset")
names(AQ_city_day)

print("Lets find the structure of the dataset")
str(AQ_city_day)

print("Lets find the summary of the dataset")
summary(AQ_city_day)

attach(AQ_city_day)

AQ_city_day [AQ_city_day == ""] <- NA

## So the air quality seems to be dependent on 12 parameters
## There are too many NAs/missing data amongst them:

## PM2.5: 4598  PM10: 11140  NO: 3582  NO2: 3585 NOx: 4185 NH3: 10328 
## CO: 2059  SO2: 3854 O3: 4022  Benzene: 5623  Toluene: 8041 Xylene: 18109
AQ_city_day %>% group_by(AQI_Bucket)%>%count()

## but third highest is NA entries...

AQ_city_day %>% group_by(AQI_Bucket)%>%count()

## Lets analyse the missing data of the dataset
n_miss(AQ_city_day) ## Total number of missing parameters
miss_var_summary(AQ_city_day) ## Missingness summary
miss_var_span(AQ_city_day, var = AQI, span_every = 250) ## Missingness spread
miss_var_table(AQ_city_day)
## vis_miss(AQ_city_day) Unable to visualise % of missing due to large data size
gg_miss_upset(AQ_city_day) ## plot for missing data
gg_miss_fct(x = AQ_city_day, fct = AQI) ## Heat map of missingness
gg_miss_span(AQ_city_day, var = AQI, span_every = 250) ## Visualize span of AQI missingness

## With this we can clearly see the AQI data is missing a lot (15%)
## There are other dataset missing heavily as well but for us AQI is the single most critical dataset
miss_scan_count(data = AQ_city_day, search = list("N/A", "NA", "na", " ","missing")) ## No empty cells

##Create shadow matrix data
head(as_shadow(AQ_city_day))

#Create nabular data by binding the shadow to the data
head(bind_shadow(AQ_city_day, only_miss = TRUE))

# Lets explore the relations ship with the missing values
AQ_city_day %>%
bind_shadow(only_miss = TRUE) %>%
group_by(AQI_NA) %>%
summarise(tCO_mean = mean(CO),CO_sd = sd(CO))

# After adding NA, there the SD and mean has also become NA

bind_shadow(AQ_city_day) %>%
ggplot(aes(x = CO,
color = AQI_NA)) +
geom_density() +
facet_wrap(~O3_NA)

# Explore the missingness in precipitation and air temperature, and display the missingness using `geom_miss_point'
ggplot(AQ_city_day, aes(x = CO,y = AQI)) + geom_miss_point()

# Looks like there are not too much of missing data

# We would like to impute all the missing data with value below the range by 10%
AQ_city_day_imp <- impute_below_all(AQ_city_day)
ggplot(AQ_city_day_imp, aes(x = CO, y = AQI)) + geom_miss_point()

# But we need to track the imputed values as well
AQ_city_day_imp_track <- bind_shadow(AQ_city_day) %>% impute_below_all()
ggplot(AQ_city_day_imp_track, aes(x = AQI, fill = AQI_NA)) + geom_histogram()
ggplot(AQ_city_day_imp_track, aes(x = O3, fill = O3_NA)) + geom_histogram()

ggplot(AQ_city_day_imp_track, aes(x = CO, y = AQI, color = AQI_NA)) + geom_point()

## So we can successfully imputed all the NA values here

# Now lets fix the important the critically missing parameters prcp and lm 
# via linear regression mechanism in relationship with other explanatory parameters
AQ_city_day_imp_lm_temp <- AQ_city_day %>% bind_shadow() %>% impute_lm(AQI ~ CO + O3) %>% impute_lm(O3 ~ CO) %>% add_label_shadow()

ggplot(AQ_city_day_imp_lm_temp, aes(x = CO, y = AQI, color = any_missing)) + geom_miss_point()

```


##Analysing AQ_city_hour: city_hour.csv
```{r}
## Have a look at the data
print ("The dimensions of the dataset")
dim(AQ_city_hour)

print("Lets have a glimpse of the dataset")
glimpse(AQ_city_hour)

print("Lets find the column names of the dataset")
names(AQ_city_hour)

print("Lets find the structure of the dataset")
str(AQ_city_hour)

print("Lets find the summary of the dataset")
summary(AQ_city_hour)

attach(AQ_city_hour)

AQ_city_hour [AQ_city_hour == ""] <- NA

## So the air quality seems to be dependent on 12 parameters
## There are too many NAs/missing data amongst them:
## PM2.5: 145088 PM10: 296737 NO: 116632   NO2: 117122  NOx: 123224 NH3:  272542     
## CO: 86517  SO2: 130373   O3: 129208 Benzene: 163646   Toluene: 220607 Xylene: 455829   
AQ_city_hour %>% group_by(AQI_Bucket)%>%count()
## Looks like Moderate entries are the highest ones, followed by Satisfactory 
## but third highest is NA entries...

n_miss(AQ_city_hour) ## Total number of missing parameters
miss_var_summary(AQ_city_hour) ## Missingness summary
miss_var_span(AQ_city_hour, var = AQI, span_every = 250) ## Missingness spread
miss_var_table(AQ_city_hour)
## vis_miss(AQ_city_hour) Unable to visualise % of missing due to large data size
gg_miss_upset(AQ_city_hour) ## plot for missing data
gg_miss_fct(x = AQ_city_hour, fct = AQI) ## Heat map of missingness
gg_miss_span(AQ_city_hour, var = AQI, span_every = 250) ## Visualize span of prcp missingness

## With this we can clearly see the precipitation data is missing a lot (39%), followed by tmin
miss_scan_count(data = AQ_city_hour, search = list("N/A", "NA", "na", " ","missing")) ## No empty cells

##Create shadow matrix data
head(as_shadow(AQ_city_hour))

#Create nabular data by binding the shadow to the data
head(bind_shadow(AQ_city_hour, only_miss = TRUE))

# Lets explore the relations ship with the missing values
AQ_city_hour %>%
bind_shadow(only_miss = TRUE) %>%
group_by(AQI_NA) %>%
summarise(tCO_mean = mean(CO),CO_sd = sd(CO))

# After adding NA, there the SD and mean has also become NA

bind_shadow(AQ_city_hour) %>%
ggplot(aes(x = CO,
color = AQI_NA)) +
geom_density() +
facet_wrap(~O3_NA)

# Explore the missingness in precipitation and air temperature, and display the missingness using `geom_miss_point'
ggplot(AQ_city_hour, aes(x = CO,y = AQI)) + geom_miss_point()

# Looks like there are not too much of missing data

# We would like to impute all the missing data with value below the range by 10%
AQ_city_hour_imp <- impute_below_all(AQ_city_hour)
ggplot(AQ_city_hour_imp, aes(x = CO, y = AQI)) + geom_miss_point()

# But we need to track the imputed values as well
AQ_city_hour_imp_track <- bind_shadow(AQ_city_hour) %>% impute_below_all()
ggplot(AQ_city_hour_imp_track, aes(x = AQI, fill = AQI_NA)) + geom_histogram()
ggplot(AQ_city_hour_imp_track, aes(x = O3, fill = O3_NA)) + geom_histogram()

ggplot(AQ_city_hour_imp_track, aes(x = CO, y = AQI, color = AQI_NA)) + geom_point()

## So we can successfully imputed all the NA values here

# Now lets fix the important the critically missing parameters prcp and lm 
# via linear regression mechanism in relationship with other explanatory parameters
AQ_city_hour_imp_lm_temp <- AQ_city_hour %>% bind_shadow() %>% impute_lm(AQI ~ CO + O3) %>% impute_lm(O3 ~ CO) %>% add_label_shadow()

ggplot(AQ_city_hour_imp_lm_temp, aes(x = CO, y = AQI, color = any_missing)) + geom_miss_point()

```
## At this point of time, we have completed preliminary analysis of the AQI datasets as well. We have witnessed plenty of missing data amongst various parameters but for us single most important data is AQI. So we have worked on both imputing all using 10% impute_all method and then imputing AQI via employing linear regression around the other important explanatory variables. After the cleaning, the dataset looks very promising to take it to next level

##Analysing Airport_delay: Aiport_Delay.csv
```{r}
## Have a look at the data

print ("The dimensions of the dataset")
dim(Airport_delay)

print("Lets have a glimpse of the dataset")
glimpse(Airport_delay)

print("Lets find the column names of the dataset")
names(Airport_delay)

print("Lets find the structure of the dataset")
str(Airport_delay)

print("Lets find the summary of the dataset")
summary(Airport_delay)

attach(Airport_delay)

Airport_delay [Airport_delay == ""] <- NA

Airport_delay %>% group_by(Departure.Airport, Departure.Airport.On.Time.Rating..out.of.10.)%>%summarize()
##Mumbai seems to have the worst rating for departure on time performance

Airport_delay %>% group_by(Arrival.Airport, Arrival.Airport.On.Time.Rating..out.of.10.)%>%summarize()
##Mumbai seems to have the worst rating for Arrival on time performance as well

```


## Further Cleaning Datasets over imputations done previously
```{r}

## Remove the entries from the table where tavg is NA
New_Weather_Bangalore <- Weather_Bangalore_imp_lm_temp[complete.cases(Weather_Bangalore),]
New_Weather_Chennai <- Weather_Chennai_imp_lm_temp[complete.cases(Weather_Chennai),]
New_Weather_Delhi <- Weather_Delhi_imp_lm_temp[complete.cases(Weather_Delhi),]
New_Weather_Lucknow <- Weather_Lucknow_imp_lm_temp[complete.cases(Weather_Lucknow),]
New_Weather_Mumbai <- Weather_Mumbai_imp_lm_temp[complete.cases(Weather_Mumbai),]
New_Weather_Jodhpur <- Weather_Jodhpur_imp_lm_temp[complete.cases(Weather_Jodhpur),]

## For Bhubhenshwar and Rourkela, we need to first remove the columns snow and tsun which has no valid entries
## We can also remove the wdir, wspd, pressure columns as the other stations are not having them
## And hence having them does not seem to add value for the scope of this analysis
Standard_Weather_Bhubhneshwar <- subset(Weather_Bhubhneshwar, select = -c(snow,wdir,wspd,pres,tsun,wpgt))
New_Weather_Bhubhneshwar <- Standard_Weather_Bhubhneshwar[complete.cases(Standard_Weather_Bhubhneshwar),] 

Standard_Weather_Rourkela <- subset(Weather_Rourkela, select = -c(snow,wdir,wspd,pres,tsun,wpgt))
New_Weather_Rourkela <- Standard_Weather_Rourkela[complete.cases(Standard_Weather_Rourkela),] 


## When it comes to AQI stations, we need only active stations

New_AQ_stations <- AQ_stations %>% filter(Status == "Active")
New_AQ_station_hour <- AQ_station_hour_imp_lm_temp[complete.cases(AQ_station_hour),]
New_AQ_station_day <- AQ_station_day_imp_lm_temp[complete.cases(AQ_station_day),]
New_AQ_city_hour <- AQ_city_hour_imp_lm_temp[complete.cases(AQ_city_hour),]
New_AQ_city_day <- AQ_city_day_imp_lm_temp[complete.cases(AQ_city_day),]

## Clean the Airport Delay data too
New_Airport_delay <- Airport_delay[complete.cases(Airport_delay),]

```
## Detailed analysis of the Datasets
## Lets try to understand the outliers a bit on the weather as they tend to impact the behaviors of the heavy precipitation which is assumed to be one of the primary reasons behind air traffic delay
```{r}
hist(x=New_Weather_Bangalore$tavg, main = "Bangalore Average Temparature")
## Data outside <20 and >30 are outliers for Bangalore average 

hist(x=New_Weather_Bangalore$tmin, main = "Bangalore Min Temparature")
## Data outside <16 are outliers for Bangalore min 

hist(x=New_Weather_Bangalore$tmax, main = "Bangalore Max  Temparature")
## Data outside >35 are outliers for Bangalore min 

hist(x=New_Weather_Bangalore$prcp, main = "Bangalore Precipitation", breaks = 5)
## Extreme cases are above 50

## So lets make special dataset
Special_Weather_Bangalore <- New_Weather_Bangalore %>% filter((tavg < 20) | (tavg>30) | (tmin < 16) | (tmax > 35) | (prcp > 50))

## And since the precipitation makes the most impact on flights, lets take a look at how precipitation gets impacted by temperatures
ggplot(Special_Weather_Bangalore, aes(x = tmin, 
                     y = tmax, 
                     color = prcp)) +
  geom_point(size = Special_Weather_Bangalore$prcp/75) +
  labs(title = "Impact of temperature on precipitation")

## From the picture looks like the extreme precipitation happens either during when tmin is between 16 to 22
## So, lets put a special filter around that and redraw
Ext_Special_Weather_Bangalore <- Special_Weather_Bangalore %>% filter((tmin > 16) & (tmin < 22))

## And since the precipitation makes the most impact on flights, lets take a look at how precipitation gets impacted by temperatures
ggplot(Ext_Special_Weather_Bangalore, aes(x = tmin, 
                     y = tmax, 
                     color = prcp)) +
  geom_point(size = Ext_Special_Weather_Bangalore$prcp/75) +
  labs(title = "Impact of temperature on precipitation")

## As per the plot, except for one heavy precipitation for a lower tmin/tmax around 18/20, most precipitations occurs when tmax is between 28 to 33 and tmin is between 19 to 22.
```
## Exploratory Analysis of Chennai Weather Dataset
```{r}
hist(x=New_Weather_Chennai$tavg, main = "Chennai Average Temparature")
## Data outside <15 and >35 are outliers for Chennai average 

hist(x=New_Weather_Chennai$tmin, main = "Chennai Min Temparature")
## Data outside <16 are outliers for Chennai min 

hist(x=New_Weather_Chennai$tmax, main = "Chennai Max  Temparature")
## Data outside >35 are outliers for Chennai min 

hist(x=New_Weather_Chennai$prcp, main = "Chennai Precipitation", breaks = 5)
## Extreme cases are above 50

## So lets make special dataset
Special_Weather_Chennai <- New_Weather_Chennai %>% filter((tavg < 15) | (tavg>35) | (tmin < 10) | (tmax > 30) | (prcp > 50))

## And since the precipitation makes the most impact on flights, lets take a look at how precipitation gets impacted by temperatures
ggplot(Special_Weather_Chennai, aes(x = tmin, 
                     y = tmax, 
                     color = prcp)) +
  geom_point(size = Special_Weather_Chennai$prcp/50) +
  labs(title = "Impact of temperature on precipitation")

## From the picture looks like the extreme precipitation happens either during when tmin is between 20 to 30
## So, lets put a special filter around that and redraw
Ext_Special_Weather_Chennai <- Special_Weather_Chennai %>% filter((tmin > 20) & (tmin < 30))

## And since the precipitation makes the most impact on flights, lets take a look at how precipitation gets impacted by temperatures
ggplot(Ext_Special_Weather_Chennai, aes(x = tmin, 
                     y = tmax, 
                     color = prcp)) +
  geom_point(size = Ext_Special_Weather_Chennai$prcp/75) +
  labs(title = "Impact of temperature on precipitation")

## As per the plot, most precipitations occurs when tmax is between 25 to 33 and tmin is between 20 to 24.
```

## Exploratory Analysis of Delhi Weather Dataset
```{r}
hist(x=New_Weather_Delhi$tavg, main = "Delhi Average Temparature")
## Data outside <15 and >35 are outliers for Delhi average 

hist(x=New_Weather_Delhi$tmin, main = "Delhi Min Temparature")
## Data outside <16 are outliers for Delhi min 

hist(x=New_Weather_Delhi$tmax, main = "Delhi Max  Temparature")
## Data outside >35 are outliers for Delhi min 

hist(x=New_Weather_Delhi$prcp, main = "Delhi Precipitation", breaks = 5)
## Extreme cases are above 50

## So lets make special dataset
Special_Weather_Delhi <- New_Weather_Delhi %>% filter((tavg < 15) | (tavg>35) | (tmin < 10) | (tmax > 30) | (prcp > 50))

## And since the precipitation makes the most impact on flights, lets take a look at how precipitation gets impacted by temperatures
ggplot(Special_Weather_Delhi, aes(x = tmin, 
                     y = tmax, 
                     color = prcp)) +
  geom_point(size = Special_Weather_Delhi$prcp/50) +
  labs(title = "Impact of temperature on precipitation")

## From the picture looks like the extreme precipitation happens either during when tmin is between 20 to 30
## So, lets put a special filter around that and redraw
Ext_Special_Weather_Delhi <- Special_Weather_Delhi %>% filter((tmin > 20) & (tmin < 30))

## And since the precipitation makes the most impact on flights, lets take a look at how precipitation gets impacted by temperatures
ggplot(Ext_Special_Weather_Delhi, aes(x = tmin, 
                     y = tmax, 
                     color = prcp)) +
  geom_point(size = Ext_Special_Weather_Delhi$prcp/75) +
  labs(title = "Impact of temperature on precipitation")

## As per the plot, most precipitations occurs when tmax is between 25 to 35 and tmin is between 23 to 27.
```


## Exploratory Analysis of Lucknow Weather Dataset
```{r}
hist(x=New_Weather_Lucknow$tavg, main = "Lucknow Average Temparature")
## Data outside <16 and >33 are outliers for Lucknow average 

hist(x=New_Weather_Lucknow$tmin, main = "Lucknow Min Temparature")
## Data outside <15 are outliers for Lucknow min 

hist(x=New_Weather_Lucknow$tmax, main = "Lucknow Max  Temparature")
## Data outside >35 are outliers for Lucknow min 

hist(x=New_Weather_Lucknow$prcp, main = "Lucknow Precipitation", breaks = 5)
## Extreme cases are above 50

## So lets make special dataset
Special_Weather_Lucknow <- New_Weather_Lucknow %>% filter((tavg < 16) | (tavg>33) | (tmin < 15) | (tmax > 30) | (prcp > 50))

## And since the precipitation makes the most impact on flights, lets take a look at how precipitation gets impacted by temperatures
ggplot(Special_Weather_Lucknow, aes(x = tmin, 
                     y = tmax, 
                     color = prcp)) +
  geom_point(size = Special_Weather_Lucknow$prcp/50) +
  labs(title = "Impact of temperature on precipitation")

## From the picture looks like the extreme precipitation happens either during when tmin is between 20 to 30
## So, lets put a special filter around that and redraw
Ext_Special_Weather_Lucknow <- Special_Weather_Lucknow %>% filter((tmin > 20) & (tmin < 30))

## And since the precipitation makes the most impact on flights, lets take a look at how precipitation gets impacted by temperatures
ggplot(Ext_Special_Weather_Lucknow, aes(x = tmin, 
                     y = tmax, 
                     color = prcp)) +
  geom_point(size = Ext_Special_Weather_Lucknow$prcp/75) +
  labs(title = "Impact of temperature on precipitation")
## As per the plot, most precipitations occurs when tmax is between 27 to 37 and tmin is between 22 to 25.
```

## Exploratory Analysis of Mumbai Weather Dataset
```{r}
hist(x=New_Weather_Mumbai$tavg, main = "Mumbai Average Temparature")
## Data outside <25 and >30 are outliers for Mumbai average 

hist(x=New_Weather_Mumbai$tmin, main = "Mumbai Min Temparature")
## Data outside <17 are outliers for Mumbai min 

hist(x=New_Weather_Mumbai$tmax, main = "Mumbai Max  Temparature")
## Data outside >35 are outliers for Mumbai min 

hist(x=New_Weather_Mumbai$prcp, main = "Mumbai Precipitation", breaks = 5)
## Extreme cases are above 50

## So lets make special dataset
Special_Weather_Mumbai <- New_Weather_Mumbai %>% filter((tavg < 25) | (tavg>30) | (tmin < 17) | (tmax > 35) | (prcp > 50))

## And since the precipitation makes the most impact on flights, lets take a look at how precipitation gets impacted by temperatures
ggplot(Special_Weather_Mumbai, aes(x = tmin, 
                     y = tmax, 
                     color = prcp)) +
  geom_point(size = Special_Weather_Mumbai$prcp/50) +
  labs(title = "Impact of temperature on precipitation")

## From the picture looks like the extreme precipitation happens either during when tmin is between 22 to 27
## So, lets put a special filter around that and redraw
Ext_Special_Weather_Mumbai <- Special_Weather_Mumbai %>% filter((tmin > 22) & (tmin < 27))

## And since the precipitation makes the most impact on flights, lets take a look at how precipitation gets impacted by temperatures
ggplot(Ext_Special_Weather_Mumbai, aes(x = tmin, 
                     y = tmax, 
                     color = prcp)) +
  geom_point(size = Ext_Special_Weather_Mumbai$prcp/75) +
  labs(title = "Impact of temperature on precipitation")
## As per the plot, most precipitations occurs when tmax is between 25 to 33 and tmin is between 22 to 26.
```

## Exploratory Analysis of Jodhpur Weather Dataset
```{r}
hist(x=New_Weather_Jodhpur$tavg, main = "Jodhpur Average Temparature")
## Data outside <22 and >28 are outliers for Jodhpur average 

hist(x=New_Weather_Jodhpur$tmin, main = "Jodhpur Min Temparature")
## Data outside <16 are outliers for Jodhpur min 

hist(x=New_Weather_Jodhpur$tmax, main = "Jodhpur Max  Temparature")
## Data outside >33 are outliers for Jodhpur min 

hist(x=New_Weather_Jodhpur$prcp, main = "Jodhpur Precipitation", breaks = 5)
## Extreme cases are above 50

## So lets make special dataset
Special_Weather_Jodhpur <- New_Weather_Jodhpur %>% filter((tavg < 22) | (tavg>28) | (tmin < 16) | (tmax > 33) | (prcp > 50))

## And since the precipitation makes the most impact on flights, lets take a look at how precipitation gets impacted by temperatures
ggplot(Special_Weather_Jodhpur, aes(x = tmin, 
                     y = tmax, 
                     color = prcp)) +
  geom_point(size = Special_Weather_Jodhpur$prcp/50) +
  labs(title = "Impact of temperature on precipitation")

## From the picture looks like the extreme precipitation happens either during when tmin is between 17 to 23
## So, lets put a special filter around that and redraw
Ext_Special_Weather_Jodhpur <- Special_Weather_Jodhpur %>% filter((tmin > 17) & (tmin < 23))

## And since the precipitation makes the most impact on flights, lets take a look at how precipitation gets impacted by temperatures
ggplot(Ext_Special_Weather_Jodhpur, aes(x = tmin, 
                     y = tmax, 
                     color = prcp)) +
  geom_point(size = Ext_Special_Weather_Jodhpur$prcp/75) +
  labs(title = "Impact of temperature on precipitation")
## As per the plot, most precipitations occurs when tmax is between 22 to 33 and tmin is between 18 to 21.
```

## Exploratory Analysis of Bhubhenshwar Weather Dataset
```{r}
hist(x=New_Weather_Bhubhneshwar$tavg, main = "Bhubhenshwar Average Temparature")
## Data outside <24 and >32 are outliers for Bhubhenshwar average 

hist(x=New_Weather_Bhubhneshwar$tmin, main = "Bhubhenshwar Min Temparature")
## Data outside <15 are outliers for Bhubhenshwar min 

hist(x=New_Weather_Bhubhneshwar$tmax, main = "Bhubhenshwar Max  Temparature")
## Data outside >35 are outliers for Bhubhenshwar min 

hist(x=New_Weather_Bhubhneshwar$prcp, main = "Bhubhenshwar Precipitation", breaks = 5)
## Extreme cases are above 50

## So lets make special dataset
Special_Weather_Bhubhenshwar <- New_Weather_Bhubhneshwar %>% filter((tavg < 24) | (tavg>32) | (tmin < 15) | (tmax > 35) | (prcp > 50))

## And since the precipitation makes the most impact on flights, lets take a look at how precipitation gets impacted by temperatures
ggplot(Special_Weather_Bhubhenshwar, aes(x = tmin, 
                     y = tmax, 
                     color = prcp)) +
  geom_point(size = Special_Weather_Bhubhenshwar$prcp/50) +
  labs(title = "Impact of temperature on precipitation")

## From the picture looks like the extreme precipitation happens either during when tmin is between 17 to 27
## So, lets put a special filter around that and redraw
Ext_Special_Weather_Bhubhenshwar <- Special_Weather_Bhubhenshwar %>% filter((tmin > 17) & (tmin < 27))

## And since the precipitation makes the most impact on flights, lets take a look at how precipitation gets impacted by temperatures
ggplot(Ext_Special_Weather_Bhubhenshwar, aes(x = tmin, 
                     y = tmax, 
                     color = prcp)) +
  geom_point(size = Ext_Special_Weather_Bhubhenshwar$prcp/75) +
  labs(title = "Impact of temperature on precipitation")
## As per the plot, most precipitations occurs when tmax is between 25 to 35 and tmin is between 22 to 25.
```

## Exploratory Analysis of Rourkela Weather Dataset
```{r}
hist(x=New_Weather_Rourkela$tavg, main = "Rourkela Average Temparature")
## Data outside <20 and >32 are outliers for Rourkela average 

hist(x=New_Weather_Rourkela$tmin, main = "Rourkela Min Temparature")
## Data outside <15 are outliers for Rourkela min 

hist(x=New_Weather_Rourkela$tmax, main = "Rourkela Max  Temparature")
## Data outside >35 are outliers for Rourkela min 

hist(x=New_Weather_Rourkela$prcp, main = "Rourkela Precipitation", breaks = 5)
## Extreme cases are above 40

## So lets make special dataset
Special_Weather_Rourkela <- New_Weather_Rourkela %>% filter((tavg < 20) | (tavg>32) | (tmin < 15) | (tmax > 30) | (prcp > 40))

## And since the precipitation makes the most impact on flights, lets take a look at how precipitation gets impacted by temperatures
ggplot(Special_Weather_Rourkela, aes(x = tmin, 
                     y = tmax, 
                     color = prcp)) +
  geom_point(size = Special_Weather_Rourkela$prcp/50) +
  labs(title = "Impact of temperature on precipitation")

## From the picture looks like the extreme precipitation happens either during when tmin is between 22 to 27
## So, lets put a special filter around that and redraw
Ext_Special_Weather_Rourkela <- Special_Weather_Rourkela %>% filter((tmin > 22) & (tmin < 27))

## And since the precipitation makes the most impact on flights, lets take a look at how precipitation gets impacted by temperatures
ggplot(Ext_Special_Weather_Rourkela, aes(x = tmin, 
                     y = tmax, 
                     color = prcp)) +
  geom_point(size = Ext_Special_Weather_Rourkela$prcp/75) +
  labs(title = "Impact of temperature on precipitation")

## Precipitation is too low quantity to derive anything meaningful here

```
## So based on all these weather data analysis, we can conclude here that precipitation is high influenced by temperature and there are more likelyhood of rain when the temperature is high - i.e., even for tmin it should be above 19 or 20 for most of the cases while tmax hovers around 330 - 35. So temperature has a important role in influencing the precipitation and hence tavg can be considered an important parameter for further modelling from here.


## Exploratory Analysis of AQI data station wise
```{r}

head(New_AQ_station_hour)
# Lets see the performance of the AQI over years
AQ_station_Day_Sep <- New_AQ_station_hour %>% separate(Datetime, c('Date', 'Time'), sep =" ") %>% separate(Time, c('Hr', 'Min', 'Sec'), sep=":") %>% mutate(Hour = as.numeric(Hr))

AQ_station_Day_Duration <- AQ_station_Day_Sep %>% mutate(Duration=cut(Hour, breaks=c(-1, 6, 18, 24),labels=c("Early_Morning","Day","Night")))

AQI_Over_Years <- AQ_station_Day_Duration%>% group_by(YEAR = year(ymd(Date)), AQI_Bucket) %>% summarize(Mean_AQI = mean(AQI))
ggplot(AQI_Over_Years, aes(x = YEAR, y = Mean_AQI, color=AQI_Bucket))+  geom_line()

## It appears that 'Severe' and 'Poor' cases didn't exist much  until 2017 from which these
## two gained at the behest of 'Good' AQI cases

# Lets see the performance of the AQI over a day in every year

AQI_Over_Time <- AQ_station_Day_Duration %>% group_by(YEAR = year(ymd(Date)), Duration) %>% summarize(Mean_AQI = mean(AQI))
ggplot(AQI_Over_Time, aes(x = YEAR, y = Mean_AQI, color = Duration))+  geom_point(size = AQI_Over_Time$Duration)

## We can see that the year 2017 had witnessed the worst air quality index but much of that
## was during the day time. Things slowed down in the years later but in them, 
## but the pattern changed by having night time pollution as the worst.
## In all cases, early morning pollution was the lowest.


# Lets see the performance of the AQI monthwise
AQI_monthwise <- AQ_station_Day_Duration %>% group_by(Month = month(ymd(Date), label = TRUE), Duration) %>% summarize(Mean_AQI = mean(AQI))
ggplot(AQI_monthwise, aes(x = Month, y = Mean_AQI, color = Duration))+  geom_point(size = AQI_monthwise$Duration)

## We can see that the colder months - i.e., from Oct to Feb, the AQI is the worst, its bad during summer but it appears the best in monsoon season.

AQI_Over_month <- AQ_station_Day_Duration %>% group_by(YEAR = year(ymd(Date)), Month = month(ymd(Date), label = TRUE), Duration) %>% summarize(Mean_AQI = mean(AQI))

# Lets see if how this works out yearwise and monthwise
ggplot(AQI_Over_month, aes(x = Month, y = Mean_AQI, color = Duration))+  geom_point(size = AQI_Over_month$Duration) + facet_wrap(~YEAR)

## We can see the same trend every year - i.e., the colder months has the worst AQI while the monsoon has the best AQI while summer/spring time having the intermediate values

## Now lets report this city wise - probably for the Month wise combination
AQI_Stationwise <- AQ_station_Day_Duration %>% group_by(Station = StationId, YEAR = year(ymd(Date)), Month = month(ymd(Date), label = TRUE), Duration) %>% summarize(Mean_AQI = mean(AQI))
ggplot(AQI_Stationwise, aes(x = Month, y = Mean_AQI, color = Station))+  geom_point(shape = AQI_Stationwise$Duration)

## Across stations, the trend seems to be the same - i.e., worst during winter, intermediate during spring/summer, best during monsoon.

## Now out of the 19 stations, we are very interested on just interested on Delhi for which we are going to do air traffic impact analysis - so lets filter them and zoom into their performance alone
AQI_Delhi_Station <- AQ_station_Day_Duration %>% filter( (StationId == "DL001") | (StationId == "DL019")) %>% group_by(Station = StationId, YEAR = year(ymd(Date)), Month = month(ymd(Date), label = TRUE), Duration) %>% summarize(Mean_AQI = mean(AQI))
ggplot(AQI_Delhi_Station, aes(x = Month, y = Mean_AQI, color = Duration))+  geom_point(size = AQI_Delhi_Station$Duration) + facet_wrap(~YEAR)

# Lets see how AQ day data is different from station hour wise data
New_AQ_station_day_Years <- New_AQ_station_day%>% group_by(YEAR = year(ymd(Date)), AQI_Bucket) %>% summarize(Mean_AQI = mean(AQI))

head(AQ_station_Day_Duration)
## There seems to be nothing new that we can derive out of the station day wise that we can't derive out of 
## station hour wise data. so no further analysis needed over here


```

## Exploratory Analysis of AQI data city wise
```{r}
## Lets look at City wise hourly AQI data
head(New_AQ_city_hour)

# Lets see the performance of the AQI over years
AQ_city_Day_Sep <- New_AQ_city_hour %>% separate(Datetime, c('Date', 'Time'), sep =" ") %>% separate(Time, c('Hr', 'Min', 'Sec'), sep=":") %>% mutate(Hour = as.numeric(Hr))

AQ_city_Day_Duration <- AQ_city_Day_Sep %>% mutate(Duration=cut(Hour, breaks=c(-1, 6, 18, 24),labels=c("Early_Morning","Day","Night")))

## Now get it grouped by Year and plot year wise performance
AQI_City_Over_Years <- AQ_city_Day_Duration%>% group_by(YEAR = year(ymd(Date)), AQI_Bucket) %>% summarize(Mean_AQI = mean(AQI))

ggplot(AQI_City_Over_Years, aes(x = YEAR, y = Mean_AQI, color=AQI_Bucket))+  geom_line()

AQI_City_Over_Time <- AQ_city_Day_Duration %>% group_by(YEAR = year(ymd(Date)), Duration) %>% summarize(Mean_AQI = mean(AQI))
ggplot(AQI_City_Over_Time, aes(x = YEAR, y = Mean_AQI, color = Duration))+  geom_point(size = AQI_City_Over_Time$Duration)

## It appears 2015 had peak values of AQIs, which dropped to very low in 2016, gained to half the levels back in 2017 and then gradually reducing
## We can see that 2015-2017 worst was during day time but from 2018, there were worse night times - may be something to do with dropped levels of AQIs as well
## In all cases, early morning pollution seems to be the lowest.


# Lets see the performance of the AQI month wise
AQI_City_monthwise <- AQ_city_Day_Duration %>% group_by(Month = month(ymd(Date), label = TRUE), Duration) %>% summarize(Mean_AQI = mean(AQI))
ggplot(AQI_City_monthwise, aes(x = Month, y = Mean_AQI, color = Duration))+  geom_point(size = AQI_City_monthwise$Duration)
## We can see that the winter months - i.e., from Oct to Feb, the AQI is the worst, its bad during summer but it appears the best in monsoon season. The difference between stationwise data is that, here Nov seems to be the worst month while in ther other dataset, Dec held the worst...

AQI_City_Over_month <- AQ_city_Day_Duration %>% group_by(YEAR = year(ymd(Date)), Month = month(ymd(Date), label = TRUE), Duration) %>% summarize(Mean_AQI = mean(AQI))
AQI_City_Over_month

# Lets see if how this works out yearwise and monthwise
ggplot(AQI_City_Over_month, aes(x = Month, y = Mean_AQI, color = Duration))+  geom_point(size = AQI_City_Over_month$Duration) + facet_wrap(~YEAR)

## We can see the same trend every year - i.e., the winter months has the worst AQI while the monsoon has the best AQI while summer/spring time having the intermediate values

## Now lets report this city wise - probably for the Month wise combination
AQI_Citywise <- AQ_city_Day_Sep %>% group_by(City, YEAR = year(ymd(Date)), Month = month(ymd(Date), label = TRUE)) %>% summarize(Mean_AQI = mean(AQI))
ggplot(AQI_Citywise, aes(x = Month, y = Mean_AQI))+  geom_point(aes(color=City))

## Across stations, the trend seems to be the same - i.e., worst during winter, intermediate during spring/summer, best during monsoon.

## Now out of all the cities, we are very interested on Delhi for which we are going to do air traffic impact analysis - so lets filter them and zoom into their performance alone
AQI_Delhi_City <- AQ_city_Day_Sep %>% filter( City == "Delhi") %>% group_by(City, YEAR = year(ymd(Date)), Month = month(ymd(Date), label = TRUE)) %>% summarize(Mean_AQI = mean(AQI))
ggplot(AQI_Delhi_City, aes(x = Month, y = Mean_AQI))+  geom_point() + facet_wrap(~YEAR)


New_AQ_city_day_Years <- New_AQ_city_day%>% group_by(City, YEAR = year(ymd(Date)), Month = month(ymd(Date), label = TRUE)) %>% summarize(Mean_AQI = mean(AQI))
head(New_AQ_city_day_Years)
ggplot(New_AQ_city_day_Years, aes(x = Month, y = Mean_AQI))+  geom_point(aes(color=City))

## There seems to be small difference when comparing hour wise data to day wise data, but not significant enough. So we will use mainly  the hour wise datafor citiwise analysis.


```
## Model for the prediction of AQI index
```{r}
## We would like to understand which of the parameters are really affecting AQI value.
## Based on the analysis above we will stick to using the Cleaned Station hour wise datasets.

New_AQ_station_hour_sep <- New_AQ_station_hour %>% separate(Datetime, c('Date', 'Time'), sep =" ") %>% separate(Time, c('Hr', 'Min', 'Sec'), sep=":") %>% mutate(Hour = as.numeric(Hr), Month = month(ymd(Date)))

## Now lets focus on the months where we have the most troubles with AQI - Oct to Feb - lets call them Bad Months
New_AQ_station_hour_sep_BM <- New_AQ_station_hour_sep %>% filter ((Month == 1) | (Month == 2) | (Month == 10) | (Month == 11) | (Month == 12))

AQI_O3_model <- lm(AQI~O3, data = New_AQ_station_hour_sep)
fmodel(AQI_O3_model)
## OK vow, looks like AQI has direct relationship with the O3 content

AQI_O3_model_BM <- lm(AQI~O3, data = New_AQ_station_hour_sep_BM)
fmodel(AQI_O3_model_BM)

## In bad months looks like O3 and AQI are inversely proportional

## Lets try with PM2.5
AQI_PM_2_5_model <- lm(AQI~PM2.5, data = New_AQ_station_hour_sep)
fmodel(AQI_PM_2_5_model)
## OK even here there is an impact - actually much more
AQI_PM_2_5_model_BM <- lm(AQI~PM2.5, data = New_AQ_station_hour_sep_BM)
fmodel(AQI_PM_2_5_model_BM)
## PM2.5 impact seems to be much higher over the winter months

##Lets try others
AQI_PM_10_model <- lm(AQI~PM10, data = New_AQ_station_hour_sep)
fmodel(AQI_PM_10_model)
AQI_PM_10_model_BM <- lm(AQI~PM10, data = New_AQ_station_hour_sep_BM)
fmodel(AQI_PM_10_model_BM)

## No significant impact change in winter months for PM10

AQI_NO_model <- lm(AQI~NO, data = New_AQ_station_hour_sep)
fmodel(AQI_NO_model)
AQI_NO_model_BM <- lm(AQI~NO, data = New_AQ_station_hour_sep_BM)
fmodel(AQI_NO_model_BM)
## Slight reduction in winter months for NO

AQI_NO2_model <- lm(AQI~NO2, data = New_AQ_station_hour_sep)
fmodel(AQI_NO2_model)
AQI_NO2_model_BM <- lm(AQI~NO2, data = New_AQ_station_hour_sep_BM)
fmodel(AQI_NO2_model_BM)
## No significant impact change in winter months for NO2

AQI_NOx_model <- lm(AQI~NOx, data = New_AQ_station_hour_sep)
fmodel(AQI_NOx_model)
AQI_NOx_model_BM <- lm(AQI~NOx, data = New_AQ_station_hour_sep_BM)
fmodel(AQI_NOx_model_BM)
## Slight reduction in winter months for NOx

AQI_NH3_model <- lm(AQI~NH3, data = New_AQ_station_hour_sep)
fmodel(AQI_NH3_model)
AQI_NH3_model_BM <- lm(AQI~NH3, data = New_AQ_station_hour_sep_BM)
fmodel(AQI_NH3_model_BM)
## NH3 impact seems to be much higher (50% more) over the winter months

AQI_CO_model <- lm(AQI~CO, data = New_AQ_station_hour_sep)
fmodel(AQI_CO_model)
AQI_CO_model_BM <- lm(AQI~CO, data = New_AQ_station_hour_sep_BM)
fmodel(AQI_CO_model_BM)
## No significant impact change in winter months for CO

AQI_SO2_model <- lm(AQI~SO2, data = New_AQ_station_hour_sep)
fmodel(AQI_SO2_model)
AQI_SO2_model_BM <- lm(AQI~SO2, data = New_AQ_station_hour_sep_BM)
fmodel(AQI_SO2_model_BM)
## Slight reduction in winter months for SO2

AQI_Benzene_model <- lm(AQI~Benzene, data = New_AQ_station_hour_sep)
fmodel(AQI_Benzene_model)
AQI_Benzene_model <- lm(AQI~Benzene, data = New_AQ_station_hour_sep_BM)
fmodel(AQI_Benzene_model)
## Slight reduction in winter months for Benzene

AQI_Toluene_model <- lm(AQI~Toluene, data = New_AQ_station_hour_sep)
fmodel(AQI_Toluene_model)
AQI_Toluene_model_BM <- lm(AQI~Toluene, data = New_AQ_station_hour_sep_BM)
fmodel(AQI_Toluene_model_BM)
## Slight reduction in winter months for Toulene

AQI_Xylene_model <- lm(AQI~Xylene, data = New_AQ_station_hour_sep)
fmodel(AQI_Xylene_model)
AQI_Xylene_model_BM <- lm(AQI~Xylene, data = New_AQ_station_hour_sep_BM)
fmodel(AQI_Xylene_model_BM)
## No significant impact change in winter months for Xylene

## Among these, the highest impact seems to be from CO. Bringing in
## O3 due to their peculiar reversal in Winter months
AQI_High_Impact_model <- lm(AQI~O3+CO, data = New_AQ_station_hour_sep)
fmodel(AQI_High_Impact_model)

AQI_High_Impact_model_BM <- lm(AQI~O3+CO, data = New_AQ_station_hour_sep_BM)
fmodel(AQI_High_Impact_model_BM)

evaluate_model(AQI_High_Impact_model)
evaluate_model(AQI_High_Impact_model_BM)

## Defintely bad months brings in a lot of differnce into the AQI behavior
## So lets consider even months as one explanatory variables
New_AQ_station_hour_sep
AQI_High_Impact_model_Month <- lm(AQI~O3+CO+month(ymd(Date)), data = New_AQ_station_hour_sep)
fmodel(AQI_High_Impact_model_Month)
evaluate_model(AQI_High_Impact_model_Month)


## Having month as part of the model really makes a difference to the evaluation.

## Now lets train the model and see if we can predict the values of AQI
#make this split reproducible
set.seed(1)

#Use 70% of dataset as training set and remaining 30% as testing set
AQI_sample_set <- sample(c(TRUE, FALSE), nrow(New_AQ_station_hour_sep), replace=TRUE, prob=c(0.7,0.3))
AQI_train_dataset  <- New_AQ_station_hour_sep[AQI_sample_set, ]
AQI_test_dataset   <- New_AQ_station_hour_sep[!AQI_sample_set, ]

AQI_Eval_model = lm(AQI~O3+CO+month(ymd(Date)), data = AQI_train_dataset)
summary(AQI_Eval_model)

Predicted_AQI_Values <- predict(AQI_Eval_model, AQI_test_dataset)

AQI_test_dataset["Predicted_AQI"] <- Predicted_AQI_Values

Summary_AQI_Model_Performace <- AQI_test_dataset %>% group_by(YEAR = year(ymd(Date)), Month) %>% summarise(AQI, Predicted_AQI)
Summary_AQI_Model_Performace

ggplot(Summary_AQI_Model_Performace, aes(x = Month)) +
        geom_point(aes(y = AQI, color = 'AQI')) +
        geom_point(aes(y = Predicted_AQI, color = 'Predictede_AQI')) +
         scale_x_continuous(breaks=seq(1, 12, by = 1))+
  labs(title = "AQI Model Performance")  + facet_wrap(~YEAR)


## We can see that there are a good amount overlaps between the AQI prediction vs actual data though
## there is still a very large scope of improvement of the model - esp when dealing with outliers.
## But so far we have sufficient proof available that AQI is heavily influenced by 
## month of the year and quantities of O3 and CO.

```

## At the end of the analysis of the AQI datasets, here are our observations 

## Form a cohesive Delhi dataset
```{r}
## We have seen how components of air impacted AQI
## Time to see the impact of weather on AQI by merging the station day wise data with the weather data
## Please note we are not picking up station hour wise data because the weather data we have is only day wise data

## Out of the cities for which weather has been provided, the only city that overlaps with the AQI data is Delhi
## And ofcourse we are trying to find the impact of AQI on Airtraffic in Delhi, so lets bring in that too
## So lets merge these three datasets only for Delhi
Delhi_AQI_data_temp <- New_AQ_station_day %>% filter ((StationId == "DL001") | (StationId == "DL019"))%>% mutate(Date_1 = ymd(as.Date(Date)))
Delhi_AQI_data <- Delhi_AQI_data_temp[, -2] %>% rename("Date" = "Date_1")
Delhi_AQI_data
New_Weather_Delhi_day <- New_Weather_Delhi %>% mutate(Date = dmy(time))

Delhi_Airport_Delay_date <- New_Airport_delay %>% filter (Departure.Airport == "DEL") %>% mutate(Date_1 = dmy(Date))
Delhi_Airport_Delay_rename <- Delhi_Airport_Delay_date[, -1] %>% rename("Date" = "Date_1")
Delhi_Airport_Delay_date_sorted <- Delhi_Airport_Delay_rename[order(Delhi_Airport_Delay_rename$Date),]

## The range of weather data is from 01/01/1990 to 25/07/2022
## The range of airport delay data is from 28/01/18 to 27/1/2020

## So the overlapping range is from 1/11/2018 to 26/1/2020

Delhi_Airport_Delay_range <- Delhi_Airport_Delay_date_sorted %>% filter (between(Date, as.Date('2018-01-25'), as.Date('2020-01-27')))
#Delhi_Airport_Delay <- Delhi_Airport_Delay_dates %>% filter ((Date >'25-01-18') & (Date < '29-01-20'))  #1925
New_Weather_Delhi_day_range <- New_Weather_Delhi_day %>% filter (between(Date, as.Date('2018-01-25'), as.Date('2020-01-27')))

Delhi_AQI_data_range <- Delhi_AQI_data %>% filter (between(Date, as.Date('2018-01-25'), as.Date('2020-01-27')))

##Delhi_Airport_Delay data has multiple entries for a day as it is cutting across many airliners operating on an airport. But we are interested in average delay per day and not really on the airliner related information. So, lets clean the data a bit there.

convert_min <- function(x)
{
  if(x < 0)
  {
    time_mins = 0
  }
  else
  {
    time_d <- hms(x)
    time_mins <- hour(time_d)*60 + minute(time_d)
  }
}

Delhi_Airport_Delay_in_min <- Delhi_Airport_Delay_range %>% mutate (Departure_Delay_min = unlist(lapply(Departure.Delay, convert_min)), Arrival_Delay_min = unlist(lapply(Arrival.Time.Delay, convert_min)))
Delhi_Airport_Delay_datewise <- Delhi_Airport_Delay_in_min  %>% group_by(Date) %>% summarize(Daily_Delay = sum(Departure_Delay_min + Arrival_Delay_min))

Delhi_AQI_weather_data_merge_temp <- merge(New_Weather_Delhi_day, Delhi_AQI_data)
Delhi_AQI_weather_data_merge_temp_1 <- Delhi_AQI_weather_data_merge_temp[,-3]
Delhi_cohesive_dataset <- merge(Delhi_AQI_weather_data_merge_temp_1, Delhi_Airport_Delay_datewise)%>% mutate(Month = month(ymd(Date)))
Delhi_cohesive_dataset
head(Delhi_cohesive_dataset)
#Looks like the merger is successful with no NA
#Lets summarize full dataset
summary(Delhi_cohesive_dataset)
```

## Analyse cohesive dataset a bit to understand how delay and other parameters plot each other
```{r}
names(Delhi_cohesive_dataset)
ggplot(Delhi_cohesive_dataset, aes(x = AQI, y = Daily_Delay, color = AQI_Bucket, size = prcp)) +
  geom_point() +
  labs(title = "Impact of AQI and prcp")

##As per the plot, Good AQI too gets observed for some delay cases but they are far and few... and does not seems to have caused high amount of delays 
## There area huge amount of delays caused for satisfactory AQI cases but most of the delays could be associated 
## with pretty high precipitation
## There are good amount of delays associate with moderate cases too and they do have caused significant delays when combined with high precipitations
## Delay instances reduces for Poor AQI cases but there is a slight increase in the values of delays
## For very poor cases, impact gets high when combined with precipitation
## Severe cases are high impact ones but looks like not affected with precipitation


## Now lets view this purely from the weather perspective
names(Delhi_cohesive_dataset)
ggplot(Delhi_cohesive_dataset, aes(x = tavg, y = Daily_Delay, color = tmin, size = prcp)) +
  geom_point() +
  labs(title = "Impact of temp and prcp")

## Its clear that bigger precipitation brings in more instances of delays
## But its also interesting to find that higher tavg, higher precipitation and higher tmin bring 
# in a lot of delays - though size of precipitation does not always result in costly delays

## Ok Lets also analyse if the components O3, PM2.5, PM10, CO, tavg, tmin, AQI has impacts on delays individually

ggplot(Delhi_cohesive_dataset, aes(x = O3, y = Daily_Delay, size = O3)) +
  geom_point() +
  labs(title = "Impact of O3")

## Looks like more O3 directly relates to higher delays

ggplot(Delhi_cohesive_dataset, aes(x = PM2.5, y = Daily_Delay, size = PM2.5)) +
  geom_point() +
  labs(title = "Impact of PM2.5")

## Looks like more PM2.5 might not have too much impact...

ggplot(Delhi_cohesive_dataset, aes(x = CO, y = Daily_Delay, size = CO)) +
  geom_point() +
  labs(title = "Impact of CO")

## Looks like size of CO has some correlation but may not be linear...

ggplot(Delhi_cohesive_dataset, aes(x = PM10, y = Daily_Delay, size = PM10)) +
  geom_point() +
  labs(title = "Impact of PM10")

## Looks like more PM2.5 might not have too much impact...

ggplot(Delhi_cohesive_dataset, aes(x = prcp, y = Daily_Delay, size = prcp)) +
  geom_point() +
  labs(title = "Impact of rain")

## Looks like amount of rain has direct impact on delays...


ggplot(Delhi_cohesive_dataset, aes(x = tavg, y = Daily_Delay, size = tavg)) +
  geom_point() +
  labs(title = "Impact of Average Temp")

## Looks like a lot of low intensity delays on higher average temprature...

ggplot(Delhi_cohesive_dataset, aes(x = tmin, y = Daily_Delay, size = tmin)) +
  geom_point() +
  labs(title = "Impact of Tmin")
## Looks like a lot of low intensity delays on higher average temprature...

ggplot(Delhi_cohesive_dataset, aes(x = AQI, y = Daily_Delay, size = AQI, color=AQI_Bucket)) +
  geom_point() +
  labs(title = "Impact of AQI")


## Looks like a lot of low intensity delays on higher Tmin...

## Lets see if the months itself has any impact on the delay

ggplot(Delhi_cohesive_dataset, aes(x = Month, y = Daily_Delay, size = Daily_Delay)) +
  geom_point()+ scale_x_continuous(breaks=seq(1, 12, by = 1))+
  labs(title = "Impact of Month")

## Looks like there is high frequency of delays during monsoon and heavy delay during peak winter season


## Ok based on this, lets pick these elements to find the right model on impacts the delays of Delhi airtraffic:
## Precipitation, AQI, tmin, O3, CO and month

## Lets see how the elements individually have linear regression relationship with the traffic delay


## Ok lets build the base model here
Delhi_Traffic_Delay_Model_AQI = lm(Daily_Delay ~ AQI, data = Delhi_cohesive_dataset)
fmodel(Delhi_Traffic_Delay_Model_AQI)

Delhi_Traffic_Delay_Model_tavg = lm(Daily_Delay ~ AQI+tavg, data = Delhi_cohesive_dataset)
fmodel(Delhi_Traffic_Delay_Model_tavg)

Delhi_Traffic_Delay_Model_prcp = lm(Daily_Delay ~ AQI+prcp, data = Delhi_cohesive_dataset)
fmodel(Delhi_Traffic_Delay_Model_prcp)

Delhi_Traffic_Delay_Model_O3 = lm(Daily_Delay ~ AQI+O3, data = Delhi_cohesive_dataset)
fmodel(Delhi_Traffic_Delay_Model_O3)

Delhi_Traffic_Delay_Model_CO = lm(Daily_Delay ~ AQI+CO, data = Delhi_cohesive_dataset)
fmodel(Delhi_Traffic_Delay_Model_CO)

Delhi_Traffic_Delay_Model_Month = lm(Daily_Delay ~ AQI+Month, data = Delhi_cohesive_dataset)
fmodel(Delhi_Traffic_Delay_Model_Month)


evaluate_model(Delhi_Traffic_Delay_Model_AQI)
evaluate_model(Delhi_Traffic_Delay_Model_tavg, tavg = 35)
evaluate_model(Delhi_Traffic_Delay_Model_prcp, prcp = 150)
evaluate_model(Delhi_Traffic_Delay_Model_O3, O3 = 50)
evaluate_model(Delhi_Traffic_Delay_Model_CO, CO = 1)
evaluate_model(Delhi_Traffic_Delay_Model_Month, Month = 12)

diff_1 <- 118.1039 - 104.8699
diff_1
diff_2 <- 114.0189 - 103.4728		
diff_2
diff_3 <- 338.1836 - 319.6457	
diff_3
diff_4 <- 148.3426 - 133.2912
diff_4
diff_5 <- 116.9036 - 97.6898	
diff_5
diff_6 <- 131.5567 - 114.9787	
diff_6
# Comparing the model evalution based on above, we can see that prcp, month and CO has good impact
# on the delay

## To evaluate the base model, split the data into test and train datasets

#make this split reproducible
set.seed(1)

#Use 70% of dataset as training set and remaining 30% as testing set
sample_set <- sample(c(TRUE, FALSE), nrow(Delhi_cohesive_dataset), replace=TRUE, prob=c(0.7,0.3))
train_dataset  <- Delhi_cohesive_dataset[sample_set, ]
test_dataset   <- Delhi_cohesive_dataset[!sample_set, ]

# the base model with just AQI and tavg
Base_Model_Delay = lm(Daily_Delay ~ AQI+prcp+CO, data = train_dataset)
# the Augmented model with precipitation as well
Aug_Model_Delay = lm(Daily_Delay ~ AQI+prcp+CO+Month, data = train_dataset)
# Run cross validation trials on the two models
trials <- cv_pred_error(Base_Model_Delay, Aug_Model_Delay)


# Compare the two sets of cross-validated errors
t.test(mse ~ model, data = trials)

# t-statistic is 2.7891. degrees of freedom, df is 6.6445 are the degrees of freedom. These are used with a t-distribution to derive p-value of 0.02842

# p-value = 0.02842 - i.e., Given that there is no actual/true difference in means, if we repeat the experiment over and over again, 2.8% of the time we would see the type of difference in means as in your samples, or a more extreme difference in means. Since p value is significantly lower than 0.05, the differences are significant.
# So we can reject the null hypothesis (H0) of no difference between the (true) averages of the two groups
#alternative hypothesis: true difference in means is not equal to 0
#95 percent confidence interval:
# 25.04453 325.43691
#If assume H0 is false, the true mean may lie in the interval [7866.835 7691.594].
# So we will chose the augmented model - i.e., Daily_Delay ~ AQI+prcp+CO+Month

```

## Model for predicting Delhi air traffic delays
```{r}
## For our model to predict the air traffic delays:
## Response Variable is Daily_Delay
## Explanatory Variables are Precipitation (prcp), AQI, CO and Month

## We are choosing a linear regression model here because this is about predicting the numerical values
## and does not belong to classification modelling
Delhi_Traffic_Delay_Model = lm(Daily_Delay ~ AQI+prcp+CO+Month, data = train_dataset)
summary(Delhi_Traffic_Delay_Model)

Predicted_Traffic_Delay <- predict(Delhi_Traffic_Delay_Model, test_dataset)
Predicted_Traffic_Delay

test_dataset["Predicted_Delay"] <- Predicted_Traffic_Delay

Summary_Model_Performace <- test_dataset %>% group_by(YEAR = year(ymd(Date)), Month) %>% summarise(Daily_Delay, Predicted_Delay)
Summary_Model_Performace

ggplot(Summary_Model_Performace, aes(x = Month)) +
        geom_point(aes(y = Daily_Delay, color = 'Daily_Delay')) +
        geom_point(aes(y = Predicted_Delay, color = 'Predictede_Delay')) +
         scale_x_continuous(breaks=seq(1, 12, by = 1))+
  labs(title = "Model Performance")  + facet_wrap(~YEAR)

# As we can see, the model is performing a bit OK for some months except for certain extreme
# cases of delays. So, the model needs further fine tuning or dataset needs to be reanalyzed.
```

##### Detailed analysis:

#  The air quality seems to be dependent on 12 parameters.
# - AQI when in worst conditions, is highest during night time, lowest during early morning 
# - AQI when not in worst conditions, is highest during day time, lowest during early morning
# - AQI were at its worst during winter, intermediate during Summer but the best during monsoon times
# - Across various stations too, the trend seems to be the same for AQI values - i.e., worst during winter, intermediate values during spring/summer, best values during monsoon.
# There seems to be small difference when comparing hour wise data to day wise data, but not significant enough. So we will live with the hour wise data itself for further AQI modelling and analysis.
 
# We would like to understand which are the parameters are really affecting AQI value. Among all the parameters, the highest difference in impact for winter seems to be from PM25.5, CO and O3. We created two linear regression models with response variable as AQI and explanatory variables as PM2.5, O3, CO and month.

# There are a good amount overlaps between the AQI prediction vs actual data though there is still a very large scope of improvement of the model - esp when dealing with outliers. But so far we have sufficient proof available that AQI is heavily influenced by month of the year and quantities of O3 and CO.

# Now we made a cohesive dataset for analysing Delhi air traffic delay and make a model for the same. We have the following observations: it appears that most of Air traffic delays are due to precipitation but AQI does impact traffic delays when it is in "Severe" category. Its also interesting to find that higher tavg, higher precipitation and higher tmin bring in a lot of delays - though size of precipitation does not always result in costly delays.

# Now the impact of various components on traffic delays individually revealed that these parameters have the highest influence on air traffic delays: Precipitation, AQI, tmin, O3, CO and month. We created two models - based model with Daily_Delay ~ AQI+prcp+CO and Augmented model with AQI+prcp+CO+Month. We then split the comprehensive Delhi_cohesive_dataset into two: 70% for training dataset and 30% for test dataset. We then calculated predicted error and conducted to sample Welch t test which gave the following results:

# t-statistic is 2.7891. degrees of freedom, df is 6.6445 are the degrees of freedom. These are used with a t-distribution to derive p-value of 0.02842. So we chose the augmented model - i.e., Daily_Delay ~ AQI+prcp+CO+Month as the final model

# For our model to predict the air traffic delays: Response Variable is Daily_Delay, Explanatory Variables are Precipitation (prcp), AQI, CO and Month. We are choosing a linear regression model here because this is about predicting the numerical values and does not belong to classification modelling. Now we trained the model with training dataset, and then predicted the delay over test dataset. We then plotted the predicted delay vs the actual delay. The model performed well except for certain extreme cases of delays. 

##### Outcomes #####

# We successfully derived the parameters that impacts the precipitation behavior in terms of quantity and temporal behavior - ie., we saw that temperature had a major role to play in precipitation and higher temperature (tmin > 19, tmax > 30) was associated with higher precipitation
# We had succesfully derived the behavioral patterms of AQI: AQI was lowest during early morning, highest during day time when AQI is high/highest during evenings when AQI is lower. AQI was worst during winter months, intermediate during summer/spring and best during monsoon times. AQI seems to be most influenced by PM2.5, O3 and CO. This was further confirmed via lm models
# We have studied the impact of weather and AQI on air traffic delays at Delhi airport. It appears that precipitation has higher impact on delays than AQI - except for "Severe" cases of AQI which directly impacts air traffic delays.
# We deduced two models for predicting Air traffic delay and chose the best model to be the one constituting of is Daily_Delay, Explanatory Variables are Precipitation (prcp), AQI, CO and Month as response variables.
# It appears that the hypothesis of Precipitation impacts Air Traffic to be true
# It appears that AQI which is having "severe" cases  during winter causes smog impacting Air Traffic - so this hypothesis is also seems to be true


##### Results and Discussions  #####


#  The air quality seems to be dependent on 12 parameters.
# From City Hour wise data set:
# - It appears that 'Severe' and 'Poor' AQI cases werent prevalent until 2017 from when these two gained at the behest of 'Good' AQI cases
# - 2017 had the worst air quality index but worst was during day time in that year
# - AQI became better after 2017 but in that duration, night time became worse than the other parts of the day.
# - In any case, early morning always featured best AQI of the day time
# - The colder months on India - i.e., from Oct to Feb, is when the AQI were at its worst; Summer had the AQI slightly better but the best time for AQI is in monsoon times. So it gives a hint that AQI might be lower when precipitation gets higher
# - We can see the same trend across the years 2015 to 2020 - i.e., the colder months had the worst AQI while the monsoon has the best AQI while summer/spring time having the intermediate values
# - Across various stations too, the trend seems to be the same for AQI values - i.e., worst during winter, intermediate values during spring/summer, best values during monsoon.
# From City Day wise data set:
# - It appears 2015 had peak values of AQIs, which dropped to very low in 2016, gained to half the levels back in 2017 and then gradually reducing in the years to follow, which is slightly different from what we have seen with City Hour wise dataset
# - We can see that during 2015-2017, worst AQI was during day time but from 2018 onwards, AQI were worse during night times - but this behavior may be influenced with overall lower levels of AQIs as well
# - But even in these cases, early morning AQI seems to be the lowest.
# - We can see that the winter months - i.e., from Oct to Feb, the AQI is the worst, its bad during summer but it appears the best in monsoon season. The difference between stationwise data is that, here Nov seems to be the worst month while in the other dataset, Dec held the worst...
# - We can see the same trend every year - i.e., the winter months has the worst AQI while the monsoon has the best AQI while summer/spring time having the intermediate values
# - Across stations, the trend seems to be the same - i.e., worst during winter, intermediate during spring/summer, best during monsoon.
# There seems to be small difference when comparing hour wise data to day wise data, but not significant enough. So we will live with the hour wise data itself for further AQI modelling and analysis.
 
# We would like to understand which are the parameters are really affecting AQI value. Based on the analysis above we used the Cleaned Station hour wise datasets. Now we focussed on the months where we have the most troubles with AQI - Oct to Feb - we called them Bad Months. Here are the observations w.r.t. impact on AQI on Bad months vs impact throughout year:
#   - In bad months looks like levels of O3 and AQI are inversely proportional
#   - PM2.5 impact seems to be much higher over the winter months
#   - No significant impact change in winter months for PM10
#   - Slight reduction in winter months for NO
#   - No significant impact change in winter months for NO2
#   - Slight reduction in winter months for NOx
#   - NH3 impact seems to be much higher (50% more) over the winter months
#   - No significant impact change in winter months for CO
#   - Slight reduction in winter months for SO2
#   - Slight reduction in winter months for Benzene
#   - Slight reduction in winter months for Toulene
#   - No significant impact change in winter months for Xylene
  
# Among all the parameters, the highest difference in impact for winter seems to be from PM25.5, CO. But we brought in O3 as well due to their peculiar inverse impact during Winter months. So we created two linear regression models with response variable as AQI and explanatory variables as PM2.5, O3 and CO - one for overall year and one for winter months. After evaluating the models defintely Bad Months brings in a lot of differnce into the AQI behavior. So we need consider months as an important explanatory variable influencing AQI.

# Now we trained the model and see if we can predict the values of AQI; Used 70% of dataset as training set and remaining 30% as testing set. We can see that there are a good amount overlaps between the AQI prediction vs actual data though there is still a very large scope of improvement of the model - esp when dealing with outliers. But so far we have sufficient proof available that AQI is heavily influenced by month of the year and quantities of O3 and CO.

# Now we made a cohesive dataset for analysing Delhi air traffic delay and make a model for the same. For this we merged the Delhi Weather dataset and AQI city day dataset (Delhi stations "DL001" and "DL019"). We merged them along the 'Date' parameters. We calculated the total delay incurred on a day - including both arrival and departure delays across the airliners. 

# We have the following observations:
# - For "Good" AQIs, a lot of delay cases but thats far and few... and does not seems to have caused 'high' amount of delays
# - Even for "Satisfactory" AQI values, huge amount of delays has been reported but most of the delays could be associated with pretty high precipitation during those days
# - There are good amount of delays associated with "Moderate" AQI values too and they do have caused significant delays when combined with high precipitations
# - Delay instances reduces for "Poor" AQI cases but there is a slight increase in the values of delays
# - For "Very poor" cases, impacts on delays gets high when combined with precipitation
# - For "Severe" cases, high impact are caused on traffic but looks like precipitation didnt play much role

# So, it appears that most of delays are due to precipitation but AQI does impact traffic delays when it is in "Severe" category. Its also interesting to find that higher tavg, higher precipitation and higher tmin bring in a lot of delays - though size of precipitation does not always result in costly delays.

# Now the impact of various components on traffic delays individually revealed that these parameters have the highest influence on air traffic delays: Precipitation, AQI, tmin, O3, CO and month. So we have built six models with AQI for the base models and the other models as augmentory ones. Further evaluation shown that AQI+prcp+CO+Month seems to have more influence. To check this up, we created two models - based model with Daily_Delay ~ AQI+prcp+CO and Augmented model with AQI+prcp+CO+Month. We then split the comprehensive Delhi_cohesive_dataset into two: 70% for training dataset and 30% for test dataset. We then calculated predicted error and conducted to sample Welch t test which gave the following results:

# t-statistic is 2.7891. degrees of freedom, df is 6.6445 are the degrees of freedom. These are used with a t-distribution to derive p-value of 0.02842

# p-value = 0.02842 - i.e., Given that there is no actual/true difference in means, if we repeat the experiment over and over again, 2.8% of the time we would see the type of difference in means as in your samples, or a more extreme difference in means. Since p value is significantly lower than 0.05, the differences are significant. So we rejected the null hypothesis (H0) of no difference between the (true) averages of the two groups alternative hypothesis: true difference in means is not equal to 0. 95 percent confidence interval: [25.04453 325.43691]. If assume H0 is false, the true mean may lie in the interval [7866.835 7691.594]. So we chose the augmented model - i.e., Daily_Delay ~ AQI+prcp+CO+Month as the final model

# For our model to predict the air traffic delays: Response Variable is Daily_Delay, Explanatory Variables are Precipitation (prcp), AQI, CO and Month. We are choosing a linear regression model here because this is about predicting the numerical values and does not belong to classification modelling. Now we trained the model with training dataset, and then predicted the delay over test dataset. We then plotted the predicted delay vs the actual delay. The model performed well except for certain extreme cases of delays. So, the model needs further fine tuning or dataset needs to be reanalyzed.
