---
title: "2023MCS120008_Ravikumar_Selvarajan_DSC_513_MiniProject"
author: "2023MCS120008_Ravikumar_Selvarajan"
date: "11/19/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(lazyeval)
library(dplyr)
library(tidyr)
library(lubridate)
library(mosaic)
library(mosaicData)
install.packages("datasets/statisticalModeling_0.3.0.tar.gz", repos = NULL, type = "source")
library(statisticalModeling)
library(naniar)
library(visdat)
library(simputation)

Weather_Station <- read.csv("./datasets/Station_GeoLocation_Longitute_Latitude_Elevation_EPSG_4326.csv")
Weather_Bangalore <- read.csv("./datasets/Bangalore_1990_2022_BangaloreCity.csv")
Weather_Chennai <- read.csv("./datasets/Chennai_1990_2022_Madras.csv")                        
Weather_Delhi <- read.csv("./datasets/Delhi_NCR_1990_2022_Safdarjung.csv")
Weather_Lucknow <- read.csv("./datasets/Lucknow_1990_2022.csv")                           
Weather_Mumbai <- read.csv("./datasets/Mumbai_1990_2022_Santacruz.csv")
Weather_Jodhpur <- read.csv("./datasets/Rajasthan_1990_2022_Jodhpur.csv")                               
Weather_Bhubhneshwar <- read.csv("./datasets/weather_Bhubhneshwar_1990_2022.csv")
Weather_Rourkela <- read.csv("./datasets/weather_Rourkela_2021_2022.csv")
AQ_stations <- read.csv("./datasets/stations.csv")                       
AQ_station_hour <- read.csv("./datasets/station_hour.csv")                                              
AQ_station_day <- read.csv("./datasets/station_day.csv")                                               
AQ_city_day <- read.csv("./datasets/city_day.csv")
AQ_city_hour <- read.csv("./datasets/city_hour.csv")                                                 
Airport_delay <- read.csv("./Aiport_Delay.csv")
```

##Exploratory Data Analysis

##Analysing Station_GeoLocation_Longitute_Latitude_Elevation_EPSG_4326.csv
```{r}
## Have a look at the data

print ("The dimensions of the dataset")
dim(Weather_Station)

print("Lets have a glimpse of the dataset")
glimpse(Weather_Station)

print("Lets find the column names of the dataset")
names(Weather_Station)

print("Lets find the structure of the dataset")
str(Weather_Station)

print("Lets find the summary of the dataset")
summary(Weather_Station)


attach(Weather_Station)
## Only Elevation seems to have some missing data, lets zoom into them
Weather_Station[is.na(Elevation),]
miss_var_summary(Weather_Station)
prop_miss(Weather_Station)
## Nothing special about why Bubhneshwar and Rourkela alone seems to have elevation missing
## No cleaning needed of its data

```
##Analysing Bangalore_1990_2022_BangaloreCity.csv
```{r}
## Have a look at the data
print ("The dimensions of the dataset")
dim(Weather_Bangalore)

print("Lets have a glimpse of the dataset")
glimpse(Weather_Bangalore)

print("Lets see the head of the dataset")
head(Weather_Bangalore)

print("Lets see the tail of the dataset")
tail(Weather_Bangalore)

print("Lets find the column names of the dataset")
all_columns <- names(Weather_Bangalore)

print("Lets find the structure of the dataset")
str(Weather_Bangalore)

print("Lets find the summary of the dataset")
summary(Weather_Bangalore)

## Lets analyse the missing data of the dataset
n_miss(Weather_Bangalore) ## Total number of missing parameters
miss_var_summary(Weather_Bangalore) ## Missingness summary
miss_var_span(Weather_Bangalore, var = prcp, span_every = 250) ## Missingness spread
miss_var_table(Weather_Bangalore)
vis_miss(Weather_Bangalore) ## visualise % of missing
gg_miss_upset(Weather_Bangalore) ## plot for missing data
gg_miss_fct(x = Weather_Bangalore, fct = prcp) ## Heat map of missingness
gg_miss_span(Weather_Bangalore, var = prcp, span_every = 250) ## Visualize span of prcp missingness

## With this we can clearly see the precipitation data is missing a lot (39%), followed by tmin
miss_scan_count(data = Weather_Bangalore, search = list("N/A", "NA", "na", " ","missing")) ## No empty cells

##Create shadow matrix data
head(as_shadow(Weather_Bangalore))

#Create nabular data by binding the shadow to the data
head(bind_shadow(Weather_Bangalore, only_miss = TRUE))

# Lets explore the relations ship with the missing values
Weather_Bangalore %>%
bind_shadow(only_miss = TRUE) %>%
group_by(prcp_NA) %>%
summarise(tavg_mean = mean(tavg),tavg_sd = sd(tavg))

# After adding NA, there the SD and mean has also become NA

bind_shadow(Weather_Bangalore) %>%
ggplot(aes(x = tavg,
color = prcp_NA)) +
geom_density() +
facet_wrap(~tmin_NA)

# Explore the missingness in precipitation and air temperature, and display the missingness using `geom_miss_point'
ggplot(Weather_Bangalore, aes(x = tavg,y = prcp)) + geom_miss_point()

ggplot(Weather_Bangalore, aes(x = tavg,y = prcp)) + geom_miss_point() +
facet_wrap(~year(dmy(time)))
# Looks like there are not too much of missing data

# We would like to impute all the missing data with value below the range by 10%
Weather_Bangalore_imp <- impute_below_all(Weather_Bangalore)
ggplot(Weather_Bangalore_imp, aes(x = tavg, y = prcp)) + geom_miss_point()

# But we need to track the imputed values as well
Weather_Bangalore_imp_track <- bind_shadow(Weather_Bangalore) %>% impute_below_all()
ggplot(Weather_Bangalore_imp_track, aes(x = prcp, fill = prcp_NA)) + geom_histogram()
ggplot(Weather_Bangalore_imp_track, aes(x = tmin, fill = tmin_NA)) + geom_histogram()

ggplot(Weather_Bangalore_imp_track, aes(x = tavg, y = prcp, color = prcp_NA)) + geom_point()

## So we can successfully imputed all the NA values here

# Now lets fix the important the critically missing parameters prcp and lm 
# via linear regression mechanism in relationship with other explanatory parameters
Wthr_Blore_imp_lm_temp <- Weather_Bangalore %>% bind_shadow() %>% impute_lm(prcp ~ tavg + tmin) %>% impute_lm(tmin ~ tavg) %>% add_label_shadow()

ggplot(Wthr_Blore_imp_lm_temp, aes(x = tavg, y = prcp, color = any_missing)) + geom_miss_point()

```


##Analysing Chennai_1990_2022_Madras.csv
```{r}
## Have a look at the data

print ("The dimensions of the dataset")
dim(Weather_Chennai)

print("Lets have a glimpse of the dataset")
glimpse(Weather_Chennai)

print("Lets see the head of the dataset")
head(Weather_Chennai)

print("Lets see the tail of the dataset")
tail(Weather_Chennai)

print("Lets find the column names of the dataset")
all_columns <- names(Weather_Chennai)

print("Lets find the structure of the dataset")
str(Weather_Chennai)

print("Lets find the summary of the dataset")
summary(Weather_Chennai)

sum(is.na(Weather_Chennai))
## About 9016 entries are NA

## Lets analyse the missing data of the dataset
n_miss(Weather_Chennai) ## Total number of missing parameters
miss_var_summary(Weather_Chennai) ## Missingness summary
miss_var_span(Weather_Chennai, var = prcp, span_every = 250) ## Missingness spread
miss_var_table(Weather_Chennai)
vis_miss(Weather_Chennai) ## visualise % of missing
gg_miss_upset(Weather_Chennai) ## plot for missing data
gg_miss_fct(x = Weather_Chennai, fct = prcp) ## Heat map of missingness
gg_miss_span(Weather_Chennai, var = prcp, span_every = 250) ## Visualize span of prcp missingness

## With this we can clearly see the precipitation data is missing a lot (39%), followed by tmin
miss_scan_count(data = Weather_Chennai, search = list("N/A", "NA", "na", " ","missing")) ## No empty cells

##Create shadow matrix data
head(as_shadow(Weather_Chennai))

#Create nabular data by binding the shadow to the data
head(bind_shadow(Weather_Chennai, only_miss = TRUE))

# Lets explore the relations ship with the missing values
Weather_Chennai %>%
bind_shadow(only_miss = TRUE) %>%
group_by(prcp_NA) %>%
summarise(tavg_mean = mean(tavg),tavg_sd = sd(tavg))

# After adding NA, there the SD and mean has also become NA

bind_shadow(Weather_Chennai) %>%
ggplot(aes(x = tavg,
color = prcp_NA)) +
geom_density() +
facet_wrap(~tmin_NA)

# Explore the missingness in precipitation and air temperature, and display the missingness using `geom_miss_point'
ggplot(Weather_Chennai, aes(x = tavg,y = prcp)) + geom_miss_point()

ggplot(Weather_Chennai, aes(x = tavg,y = prcp)) + geom_miss_point() +
facet_wrap(~year(dmy(time)))
# Looks like there are not too much of missing data

# We would like to impute all the missing data with value below the range by 10%
Weather_Chennai_imp <- impute_below_all(Weather_Chennai)
ggplot(Weather_Chennai_imp, aes(x = tavg, y = prcp)) + geom_miss_point()

# But we need to track the imputed values as well
Weather_Chennai_imp_track <- bind_shadow(Weather_Chennai) %>% impute_below_all()
ggplot(Weather_Chennai_imp_track, aes(x = prcp, fill = prcp_NA)) + geom_histogram()
ggplot(Weather_Chennai_imp_track, aes(x = tmin, fill = tmin_NA)) + geom_histogram()

ggplot(Weather_Chennai_imp_track, aes(x = tavg, y = prcp, color = prcp_NA)) + geom_point()

## So we can successfully imputed all the NA values here

# Now lets fix the important the critically missing parameters prcp and lm 
# via linear regression mechanism in relationship with other explanatory parameters
Weather_Chennai_imp_lm_temp <- Weather_Chennai %>% bind_shadow() %>% impute_lm(prcp ~ tavg + tmin) %>% impute_lm(tmin ~ tavg) %>% add_label_shadow()

ggplot(Weather_Chennai_imp_lm_temp, aes(x = tavg, y = prcp, color = any_missing)) + geom_miss_point()

```

##Analysing Delhi_NCR_1990_2022_Safdarjung.csv
```{r}
## Have a look at the data

print ("The dimensions of the dataset")
dim(Weather_Delhi)

print("Lets have a glimpse of the dataset")
glimpse(Weather_Delhi)

print("Lets see the head of the dataset")
head(Weather_Delhi)

print("Lets see the tail of the dataset")
tail(Weather_Delhi)

print("Lets find the column names of the dataset")
all_columns <- names(Weather_Delhi)

print("Lets find the structure of the dataset")
str(Weather_Delhi)

print("Lets find the summary of the dataset")
summary(Weather_Delhi)

sum(is.na(Weather_Delhi))
## About 8303 entries are NA
## Lets analyse the missing data of the dataset
n_miss(Weather_Delhi) ## Total number of missing parameters
miss_var_summary(Weather_Delhi) ## Missingness summary
miss_var_span(Weather_Delhi, var = prcp, span_every = 250) ## Missingness spread
miss_var_table(Weather_Delhi)
vis_miss(Weather_Delhi) ## visualise % of missing
gg_miss_upset(Weather_Delhi) ## plot for missing data
gg_miss_fct(x = Weather_Delhi, fct = prcp) ## Heat map of missingness
gg_miss_span(Weather_Delhi, var = prcp, span_every = 250) ## Visualize span of prcp missingness

## With this we can clearly see the precipitation data is missing a lot (39%), followed by tmin
miss_scan_count(data = Weather_Delhi, search = list("N/A", "NA", "na", " ","missing")) ## No empty cells

##Create shadow matrix data
head(as_shadow(Weather_Delhi))

#Create nabular data by binding the shadow to the data
head(bind_shadow(Weather_Delhi, only_miss = TRUE))

# Lets explore the relations ship with the missing values
Weather_Delhi %>%
bind_shadow(only_miss = TRUE) %>%
group_by(prcp_NA) %>%
summarise(tavg_mean = mean(tavg),tavg_sd = sd(tavg))

# After adding NA, there the SD and mean has also become NA

bind_shadow(Weather_Delhi) %>%
ggplot(aes(x = tavg,
color = prcp_NA)) +
geom_density() +
facet_wrap(~tmin_NA)

# Explore the missingness in precipitation and air temperature, and display the missingness using `geom_miss_point'
ggplot(Weather_Delhi, aes(x = tavg,y = prcp)) + geom_miss_point()

ggplot(Weather_Delhi, aes(x = tavg,y = prcp)) + geom_miss_point() +
facet_wrap(~year(dmy(time)))
# Looks like there are not too much of missing data

# We would like to impute all the missing data with value below the range by 10%
Weather_Delhi_imp <- impute_below_all(Weather_Delhi)
ggplot(Weather_Delhi_imp, aes(x = tavg, y = prcp)) + geom_miss_point()

# But we need to track the imputed values as well
Weather_Delhi_imp_track <- bind_shadow(Weather_Delhi) %>% impute_below_all()
ggplot(Weather_Delhi_imp_track, aes(x = prcp, fill = prcp_NA)) + geom_histogram()
ggplot(Weather_Delhi_imp_track, aes(x = tmin, fill = tmin_NA)) + geom_histogram()

ggplot(Weather_Delhi_imp_track, aes(x = tavg, y = prcp, color = prcp_NA)) + geom_point()

## So we can successfully imputed all the NA values here

# Now lets fix the important the critically missing parameters prcp and lm 
# via linear regression mechanism in relationship with other explanatory parameters
Weather_Delhi_imp_lm_temp <- Weather_Delhi %>% bind_shadow() %>% impute_lm(prcp ~ tavg + tmin) %>% impute_lm(tmin ~ tavg) %>% add_label_shadow()

ggplot(Weather_Delhi_imp_lm_temp, aes(x = tavg, y = prcp, color = any_missing)) + geom_miss_point()


```

##Analysing Lucknow_1990_2022.csv
```{r}
## Have a look at the data

print ("The dimensions of the dataset")
dim(Weather_Lucknow)

print("Lets have a glimpse of the dataset")
glimpse(Weather_Lucknow)

print("Lets see the head of the dataset")
head(Weather_Lucknow)

print("Lets see the tail of the dataset")
tail(Weather_Lucknow)

print("Lets find the column names of the dataset")
all_columns <- names(Weather_Lucknow)

print("Lets find the structure of the dataset")
str(Weather_Lucknow)

print("Lets find the summary of the dataset")
summary(Weather_Lucknow)

sum(is.na(Weather_Lucknow))
## About 11358 entries are NA

## Lets analyse the missing data of the dataset
n_miss(Weather_Lucknow) ## Total number of missing parameters
miss_var_summary(Weather_Lucknow) ## Missingness summary
miss_var_span(Weather_Lucknow, var = prcp, span_every = 250) ## Missingness spread
miss_var_table(Weather_Lucknow)
vis_miss(Weather_Lucknow) ## visualise % of missing
gg_miss_upset(Weather_Lucknow) ## plot for missing data
gg_miss_fct(x = Weather_Lucknow, fct = prcp) ## Heat map of missingness
gg_miss_span(Weather_Lucknow, var = prcp, span_every = 250) ## Visualize span of prcp missingness

## With this we can clearly see the precipitation data is missing a lot (39%), followed by tmin
miss_scan_count(data = Weather_Lucknow, search = list("N/A", "NA", "na", " ","missing")) ## No empty cells

##Create shadow matrix data
head(as_shadow(Weather_Lucknow))

#Create nabular data by binding the shadow to the data
head(bind_shadow(Weather_Lucknow, only_miss = TRUE))

# Lets explore the relations ship with the missing values
Weather_Lucknow %>%
bind_shadow(only_miss = TRUE) %>%
group_by(prcp_NA) %>%
summarise(tavg_mean = mean(tavg),tavg_sd = sd(tavg))

# After adding NA, there the SD and mean has also become NA

bind_shadow(Weather_Lucknow) %>%
ggplot(aes(x = tavg,
color = prcp_NA)) +
geom_density() +
facet_wrap(~tmin_NA)

# Explore the missingness in precipitation and air temperature, and display the missingness using `geom_miss_point'
ggplot(Weather_Lucknow, aes(x = tavg,y = prcp)) + geom_miss_point()

ggplot(Weather_Lucknow, aes(x = tavg,y = prcp)) + geom_miss_point() +
facet_wrap(~year(dmy(time)))
# Looks like there are not too much of missing data

# We would like to impute all the missing data with value below the range by 10%
Weather_Lucknow_imp <- impute_below_all(Weather_Lucknow)
ggplot(Weather_Lucknow_imp, aes(x = tavg, y = prcp)) + geom_miss_point()

# But we need to track the imputed values as well
Weather_Lucknow_imp_track <- bind_shadow(Weather_Lucknow) %>% impute_below_all()
ggplot(Weather_Lucknow_imp_track, aes(x = prcp, fill = prcp_NA)) + geom_histogram()
ggplot(Weather_Lucknow_imp_track, aes(x = tmin, fill = tmin_NA)) + geom_histogram()

ggplot(Weather_Lucknow_imp_track, aes(x = tavg, y = prcp, color = prcp_NA)) + geom_point()

## So we can successfully imputed all the NA values here

# Now lets fix the important the critically missing parameters prcp and lm 
# via linear regression mechanism in relationship with other explanatory parameters
Weather_Lucknow_imp_lm_temp <- Weather_Lucknow %>% bind_shadow() %>% impute_lm(prcp ~ tavg + tmin) %>% impute_lm(tmin ~ tavg) %>% add_label_shadow()

ggplot(Weather_Lucknow_imp_lm_temp, aes(x = tavg, y = prcp, color = any_missing)) + geom_miss_point()

```

##Analysing Mumbai_1990_2022_Santacruz.csv
```{r}
## Have a look at the data

print ("The dimensions of the dataset")
dim(Weather_Mumbai)

print("Lets have a glimpse of the dataset")
glimpse(Weather_Mumbai)

print("Lets see the head of the dataset")
head(Weather_Mumbai)

print("Lets see the tail of the dataset")
tail(Weather_Mumbai)

print("Lets find the column names of the dataset")
all_columns <- names(Weather_Mumbai)

print("Lets find the structure of the dataset")
str(Weather_Mumbai)

print("Lets find the summary of the dataset")
summary(Weather_Mumbai)

sum(is.na(Weather_Mumbai))
## About 9053 entries are NA

## Lets analyse the missing data of the dataset
n_miss(Weather_Mumbai) ## Total number of missing parameters
miss_var_summary(Weather_Mumbai) ## Missingness summary
miss_var_span(Weather_Mumbai, var = prcp, span_every = 250) ## Missingness spread
miss_var_table(Weather_Mumbai)
vis_miss(Weather_Mumbai) ## visualise % of missing
gg_miss_upset(Weather_Mumbai) ## plot for missing data
gg_miss_fct(x = Weather_Mumbai, fct = prcp) ## Heat map of missingness
gg_miss_span(Weather_Mumbai, var = prcp, span_every = 250) ## Visualize span of prcp missingness

## With this we can clearly see the precipitation data is missing a lot (39%), followed by tmin
miss_scan_count(data = Weather_Mumbai, search = list("N/A", "NA", "na", " ","missing")) ## No empty cells

##Create shadow matrix data
head(as_shadow(Weather_Mumbai))

#Create nabular data by binding the shadow to the data
head(bind_shadow(Weather_Mumbai, only_miss = TRUE))

# Lets explore the relations ship with the missing values
Weather_Mumbai %>%
bind_shadow(only_miss = TRUE) %>%
group_by(prcp_NA) %>%
summarise(tavg_mean = mean(tavg),tavg_sd = sd(tavg))

# After adding NA, there the SD and mean has also become NA

bind_shadow(Weather_Mumbai) %>%
ggplot(aes(x = tavg,
color = prcp_NA)) +
geom_density() +
facet_wrap(~tmin_NA)

# Explore the missingness in precipitation and air temperature, and display the missingness using `geom_miss_point'
ggplot(Weather_Mumbai, aes(x = tavg,y = prcp)) + geom_miss_point()

ggplot(Weather_Mumbai, aes(x = tavg,y = prcp)) + geom_miss_point() +
facet_wrap(~year(dmy(time)))
# Looks like there are not too much of missing data

# We would like to impute all the missing data with value below the range by 10%
Weather_Mumbai_imp <- impute_below_all(Weather_Mumbai)
ggplot(Weather_Mumbai_imp, aes(x = tavg, y = prcp)) + geom_miss_point()

# But we need to track the imputed values as well
Weather_Mumbai_imp_track <- bind_shadow(Weather_Mumbai) %>% impute_below_all()
ggplot(Weather_Mumbai_imp_track, aes(x = prcp, fill = prcp_NA)) + geom_histogram()
ggplot(Weather_Mumbai_imp_track, aes(x = tmin, fill = tmin_NA)) + geom_histogram()

ggplot(Weather_Mumbai_imp_track, aes(x = tavg, y = prcp, color = prcp_NA)) + geom_point()

## So we can successfully imputed all the NA values here

# Now lets fix the important the critically missing parameters prcp and lm 
# via linear regression mechanism in relationship with other explanatory parameters
Weather_Mumbai_imp_lm_temp <- Weather_Mumbai %>% bind_shadow() %>% impute_lm(prcp ~ tavg + tmin) %>% impute_lm(tmin ~ tavg) %>% add_label_shadow()

ggplot(Weather_Mumbai_imp_lm_temp, aes(x = tavg, y = prcp, color = any_missing)) + geom_miss_point()

```

##Analysing Rajasthan_1990_2022_Jodhpur.csv
```{r}
## Have a look at the data


print ("The dimensions of the dataset")
dim(Weather_Jodhpur)

print("Lets have a glimpse of the dataset")
glimpse(Weather_Jodhpur)

print("Lets see the head of the dataset")
head(Weather_Jodhpur)

print("Lets see the tail of the dataset")
tail(Weather_Jodhpur)

print("Lets find the column names of the dataset")
all_columns <- names(Weather_Jodhpur)

print("Lets find the structure of the dataset")
str(Weather_Jodhpur)

print("Lets find the summary of the dataset")
summary(Weather_Jodhpur)

sum(is.na(Weather_Jodhpur))
## About 6708 entries are NA

## Lets analyse the missing data of the dataset
n_miss(Weather_Jodhpur) ## Total number of missing parameters
miss_var_summary(Weather_Jodhpur) ## Missingness summary
miss_var_span(Weather_Jodhpur, var = prcp, span_every = 250) ## Missingness spread
miss_var_table(Weather_Jodhpur)
vis_miss(Weather_Jodhpur) ## visualise % of missing
gg_miss_upset(Weather_Jodhpur) ## plot for missing data
gg_miss_fct(x = Weather_Jodhpur, fct = prcp) ## Heat map of missingness
gg_miss_span(Weather_Jodhpur, var = prcp, span_every = 250) ## Visualize span of prcp missingness

## With this we can clearly see the precipitation data is missing a lot (39%), followed by tmin
miss_scan_count(data = Weather_Jodhpur, search = list("N/A", "NA", "na", " ","missing")) ## No empty cells

##Create shadow matrix data
head(as_shadow(Weather_Jodhpur))

#Create nabular data by binding the shadow to the data
head(bind_shadow(Weather_Jodhpur, only_miss = TRUE))

# Lets explore the relations ship with the missing values
Weather_Jodhpur %>%
bind_shadow(only_miss = TRUE) %>%
group_by(prcp_NA) %>%
summarise(tavg_mean = mean(tavg),tavg_sd = sd(tavg))

# After adding NA, there the SD and mean has also become NA

bind_shadow(Weather_Jodhpur) %>%
ggplot(aes(x = tavg,
color = prcp_NA)) +
geom_density() +
facet_wrap(~tmin_NA)

# Explore the missingness in precipitation and air temperature, and display the missingness using `geom_miss_point'
ggplot(Weather_Jodhpur, aes(x = tavg,y = prcp)) + geom_miss_point()

ggplot(Weather_Jodhpur, aes(x = tavg,y = prcp)) + geom_miss_point() +
facet_wrap(~year(dmy(time)))
# Looks like there are not too much of missing data

# We would like to impute all the missing data with value below the range by 10%
Weather_Jodhpur_imp <- impute_below_all(Weather_Jodhpur)
ggplot(Weather_Jodhpur_imp, aes(x = tavg, y = prcp)) + geom_miss_point()

# But we need to track the imputed values as well
Weather_Jodhpur_imp_track <- bind_shadow(Weather_Jodhpur) %>% impute_below_all()
ggplot(Weather_Jodhpur_imp_track, aes(x = prcp, fill = prcp_NA)) + geom_histogram()
ggplot(Weather_Jodhpur_imp_track, aes(x = tmin, fill = tmin_NA)) + geom_histogram()

ggplot(Weather_Jodhpur_imp_track, aes(x = tavg, y = prcp, color = prcp_NA)) + geom_point()

## So we can successfully imputed all the NA values here

# Now lets fix the important the critically missing parameters prcp and lm 
# via linear regression mechanism in relationship with other explanatory parameters
Weather_Jodhpur_imp_lm_temp <- Weather_Jodhpur %>% bind_shadow() %>% impute_lm(prcp ~ tavg + tmin) %>% impute_lm(tmin ~ tavg) %>% add_label_shadow()

ggplot(Weather_Jodhpur_imp_lm_temp, aes(x = tavg, y = prcp, color = any_missing)) + geom_miss_point()

```

##Analysing weather_Bhubhneshwar_1990_2022.csv")
```{r}
## Have a look at the data

#definitely has more columns than the cities that we have seen so far

print ("The dimensions of the dataset")
dim(Weather_Bhubhneshwar)
#OK, so we have 11 columns, 6 more than others

print("Lets have a glimpse of the dataset")
glimpse(Weather_Bhubhneshwar)

print("Lets see the head of the dataset")
head(Weather_Bhubhneshwar)

print("Lets see the tail of the dataset")
tail(Weather_Bhubhneshwar)

print("Lets find the column names of the dataset")
names(Weather_Bhubhneshwar)

## So the additional columns are: snow, wind direction, wind speed, wind pgt, pressure and tsunami

print("Lets find the structure of the dataset")
str(Weather_Bhubhneshwar)

print("Lets find the summary of the dataset")
summary(Weather_Bhubhneshwar)

sum(is.na(Weather_Bhubhneshwar))
## About 75100 entries are NA
## Lets analyse the missing data of the dataset
n_miss(Weather_Bhubhneshwar) ## Total number of missing parameters
miss_var_summary(Weather_Bhubhneshwar) ## Missingness summary
miss_var_span(Weather_Bhubhneshwar, var = prcp, span_every = 250) ## Missingness spread
miss_var_table(Weather_Bhubhneshwar)
vis_miss(Weather_Bhubhneshwar) ## visualise % of missing
gg_miss_upset(Weather_Bhubhneshwar) ## plot for missing data
gg_miss_fct(x = Weather_Bhubhneshwar, fct = prcp) ## Heat map of missingness
gg_miss_span(Weather_Bhubhneshwar, var = prcp, span_every = 250) ## Visualize span of prcp missingness

## With this we can clearly see the precipitation data is missing a lot (39%), followed by tmin
miss_scan_count(data = Weather_Bhubhneshwar, search = list("N/A", "NA", "na", " ","missing")) ## No empty cells

##Create shadow matrix data
head(as_shadow(Weather_Bhubhneshwar))

#Create nabular data by binding the shadow to the data
head(bind_shadow(Weather_Bhubhneshwar, only_miss = TRUE))

# Lets explore the relations ship with the missing values
Weather_Bhubhneshwar %>%
bind_shadow(only_miss = TRUE) %>%
group_by(prcp_NA) %>%
summarise(tavg_mean = mean(tavg),tavg_sd = sd(tavg))

# After adding NA, there the SD and mean has also become NA

bind_shadow(Weather_Bhubhneshwar) %>%
ggplot(aes(x = tavg,
color = prcp_NA)) +
geom_density() +
facet_wrap(~tmin_NA)

# Explore the missingness in precipitation and air temperature, and display the missingness using `geom_miss_point'
ggplot(Weather_Bhubhneshwar, aes(x = tavg,y = prcp)) + geom_miss_point()

ggplot(Weather_Bhubhneshwar, aes(x = tavg,y = prcp)) + geom_miss_point() +
facet_wrap(~year(dmy(time)))
# Looks like there are not too much of missing data

```

##Analysing weather_Rourkela_2021_2022.csv")
```{r}
## Have a look at the data
#definitely has more columns than the cities that we have seen so far

print ("The dimensions of the dataset")
dim(Weather_Rourkela)
#OK, so we have 11 columns, 6 more than others

print("Lets have a glimpse of the dataset")
glimpse(Weather_Rourkela)

print("Lets see the head of the dataset")
head(Weather_Rourkela)

print("Lets see the tail of the dataset")
tail(Weather_Rourkela)

print("Lets find the column names of the dataset")
names(Weather_Rourkela)

## So the additional columns are: snow, wind direction, wind speed, wind pgt, pressure and tsunami

print("Lets find the structure of the dataset")
str(Weather_Rourkela)

print("Lets find the summary of the dataset")
summary(Weather_Rourkela)

sum(is.na(Weather_Rourkela))
## About 1293 entries are NA

## Lets analyse the missing data of the dataset
n_miss(Weather_Rourkela) ## Total number of missing parameters
miss_var_summary(Weather_Rourkela) ## Missingness summary
miss_var_span(Weather_Rourkela, var = prcp, span_every = 250) ## Missingness spread
miss_var_table(Weather_Rourkela)
vis_miss(Weather_Rourkela) ## visualise % of missing
gg_miss_upset(Weather_Rourkela) ## plot for missing data
gg_miss_fct(x = Weather_Rourkela, fct = prcp) ## Heat map of missingness
gg_miss_span(Weather_Rourkela, var = prcp, span_every = 250) ## Visualize span of prcp missingness

## With this we can clearly see the precipitation data is missing a lot (39%), followed by tmin
Weather_Rourkela
miss_scan_count(data = Weather_Rourkela, search = list("N/A", "NA", "na", " ","missing")) ## No empty cells

##Create shadow matrix data
as_shadow(Weather_Rourkela)

#Create nabular data by binding the shadow to the data
bind_shadow(Weather_Rourkela, only_miss = TRUE)

# Lets explore the relations ship with the missing values
Weather_Rourkela %>%
bind_shadow(only_miss = TRUE) %>%
group_by(prcp_NA) %>%
summarise(tavg_mean = mean(tavg),tavg_sd = sd(tavg))

# After adding NA, there the SD and mean has also become NA

bind_shadow(Weather_Rourkela) %>%
ggplot(aes(x = tavg,
color = prcp_NA)) +
geom_density() +
facet_wrap(~tmin_NA)

# Explore the missingness in precipitation and air temperature, and display the missingness using `geom_miss_point'
ggplot(Weather_Rourkela, aes(x = tavg,y = prcp)) + geom_miss_point()

ggplot(Weather_Rourkela, aes(x = tavg,y = prcp)) + geom_miss_point() +
facet_wrap(~year(dmy(time)))
# Looks like there are not too much of missing data

```




##Analysing AQI stations: stations.csv
```{r}
## Have a look at the data

print ("The dimensions of the dataset")
dim(AQ_stations)

print("Lets have a glimpse of the dataset")
glimpse(AQ_stations)

print("Lets find the column names of the dataset")
names(AQ_stations)

print("Lets find the structure of the dataset")
str(AQ_stations)

print("Lets find the summary of the dataset")
summary(AQ_stations)


attach(AQ_stations)

AQ_stations [AQ_stations == ""] <- NA
## There is no records with NA but there are records with missing data.
## Lets fill them with NA and then find it.
AQ_stations[is.na(Status),]
```
##Analysing AQI Station Hour wise - station_hour.csv
```{r}
## Have a look at the data

print ("The dimensions of the dataset")
dim(AQ_station_hour)

print("Lets have a glimpse of the dataset")
glimpse(AQ_station_hour)

print("Lets find the column names of the dataset")
names(AQ_station_hour)

print("Lets find the structure of the dataset")
str(AQ_station_hour)

print("Lets find the summary of the dataset")
summary(AQ_station_hour)

attach(AQ_station_hour)

AQ_station_hour [AQ_station_hour == ""] <- NA

## So the air quality seems to be dependent on 12 parameters
## There are too many NAs/missing data amongst them:
## PM2.5:647689  PM10:1119252 NO:553711       NO2:528973      NOx:490808  CO:1236618 
## SO2:499302    O3:742737    Benzene:725973  Toluene:861579  Xylene:1042366

AQ_station_hour %>% group_by(AQI_Bucket)%>%count()
## Looks like Moderate entries are the highest ones but second highest is NA entries...
## Lets analyse the missing data of the dataset
n_miss(AQ_station_hour) ## Total number of missing parameters
miss_var_summary(AQ_station_hour) ## Missingness summary
miss_var_span(AQ_station_hour, var = AQI, span_every = 250) ## Missingness spread
miss_var_table(AQ_station_hour)
## vis_miss(AQ_station_hour) Unable to visualise % of missing due to large data size
gg_miss_upset(AQ_station_hour) ## plot for missing data
gg_miss_fct(x = AQ_station_hour, fct = AQI) ## Heat map of missingness
gg_miss_span(AQ_station_hour, var = AQI, span_every = 250) ## Visualize span of prcp missingness

## With this we can clearly see the precipitation data is missing a lot (39%), followed by tmin
miss_scan_count(data = AQ_station_hour, search = list("N/A", "NA", "na", " ","missing")) ## No empty cells

##Create shadow matrix data
head(as_shadow(AQ_station_hour))

#Create nabular data by binding the shadow to the data
head(bind_shadow(AQ_station_hour, only_miss = TRUE))

# Lets explore the relations ship with the missing values
AQ_station_hour %>%
bind_shadow(only_miss = TRUE) %>%
group_by(AQI_NA) %>%
summarise(tCO_mean = mean(CO),CO_sd = sd(CO))

# After adding NA, there the SD and mean has also become NA

bind_shadow(AQ_station_hour) %>%
ggplot(aes(x = CO,
color = AQI_NA)) +
geom_density() +
facet_wrap(~NO3_NA)

# Explore the missingness in precipitation and air temperature, and display the missingness using `geom_miss_point'
ggplot(AQ_station_hour, aes(x = CO,y = AQI)) + geom_miss_point()

# Looks like there are not too much of missing data

# We would like to impute all the missing data with value below the range by 10%
AQ_station_hour_imp <- impute_below_all(AQ_station_hour)
ggplot(AQ_station_hour_imp, aes(x = CO, y = AQI)) + geom_miss_point()

# But we need to track the imputed values as well
AQ_station_hour_imp_track <- bind_shadow(AQ_station_hour) %>% impute_below_all()
ggplot(AQ_station_hour_imp_track, aes(x = AQI, fill = AQI_NA)) + geom_histogram()
ggplot(AQ_station_hour_imp_track, aes(x = O3, fill = O3_NA)) + geom_histogram()

ggplot(AQ_station_hour_imp_track, aes(x = CO, y = AQI, color = AQI_NA)) + geom_point()

## So we can successfully imputed all the NA values here

# Now lets fix the important the critically missing parameters prcp and lm 
# via linear regression mechanism in relationship with other explanatory parameters
AQ_station_hour_imp_lm_temp <- AQ_station_hour %>% bind_shadow() %>% impute_lm(AQI ~ CO + O3) %>% impute_lm(O3 ~ CO) %>% add_label_shadow()

ggplot(AQ_station_hour_imp_lm_temp, aes(x = CO, y = AQI, color = any_missing)) + geom_miss_point()

```

##Analysing AQ_station_day  - station_day.csv
```{r}
## Have a look at the data

print ("The dimensions of the dataset")
dim(AQ_station_day)

print("Lets have a glimpse of the dataset")
glimpse(AQ_station_day)

print("Lets find the column names of the dataset")
names(AQ_station_day)

print("Lets find the structure of the dataset")
str(AQ_station_day)

print("Lets find the summary of the dataset")
summary(AQ_station_day)

attach(AQ_station_day)

AQ_station_day [AQ_station_day == ""] <- NA

## So the air quality seems to be dependent on 12 parameters
## There are too many NAs/missing data amongst them:
## PM2.5: 21625 PM10: 42706 NO: 17106 NO2: 16547 NOx: 15500 NH3: 48105 
## CO:  12998  SO2: 25204  O3: 25568 Benzene: 31455 Toluene: 38702 Xylene: 85137
AQ_station_day %>% group_by(AQI_Bucket)%>%count()
## Looks like Moderate entries are the highest ones, followed by Satisfactory 
## but third highest is NA entries...

```
##Analysing AQ_city_day <- read.csv("./datasets/city_day.csv")
```{r}
## Have a look at the data

print ("The dimensions of the dataset")
dim(AQ_city_day)

print("Lets have a glimpse of the dataset")
glimpse(AQ_city_day)

print("Lets find the column names of the dataset")
names(AQ_city_day)

print("Lets find the structure of the dataset")
str(AQ_city_day)

print("Lets find the summary of the dataset")
summary(AQ_city_day)

attach(AQ_city_day)

AQ_city_day [AQ_city_day == ""] <- NA

## So the air quality seems to be dependent on 12 parameters
## There are too many NAs/missing data amongst them:

## PM2.5: 4598  PM10: 11140  NO: 3582  NO2: 3585 NOx: 4185 NH3: 10328 
## CO: 2059  SO2: 3854 O3: 4022  Benzene: 5623  Toluene: 8041 Xylene: 18109
AQ_city_day %>% group_by(AQI_Bucket)%>%count()
## Looks like Moderate entries are the highest ones, followed by Satisfactory 
## but third highest is NA entries...
```


##Analysing AQ_city_hour: city_hour.csv
```{r}
## Have a look at the data
print ("The dimensions of the dataset")
dim(AQ_city_hour)

print("Lets have a glimpse of the dataset")
glimpse(AQ_city_hour)

print("Lets find the column names of the dataset")
names(AQ_city_hour)

print("Lets find the structure of the dataset")
str(AQ_city_hour)

print("Lets find the summary of the dataset")
summary(AQ_city_hour)

attach(AQ_city_hour)

AQ_city_hour [AQ_city_hour == ""] <- NA

## So the air quality seems to be dependent on 12 parameters
## There are too many NAs/missing data amongst them:
## PM2.5: 145088 PM10: 296737 NO: 116632   NO2: 117122  NOx: 123224 NH3:  272542     
## CO: 86517  SO2: 130373   O3: 129208 Benzene: 163646   Toluene: 220607 Xylene: 455829   
AQ_city_hour %>% group_by(AQI_Bucket)%>%count()
## Looks like Moderate entries are the highest ones, followed by Satisfactory 
## but third highest is NA entries...
```

##Analysing Airport_delay: Aiport_Delay.csv
```{r}
## Have a look at the data

print ("The dimensions of the dataset")
dim(Airport_delay)

print("Lets have a glimpse of the dataset")
glimpse(Airport_delay)

print("Lets find the column names of the dataset")
names(Airport_delay)

print("Lets find the structure of the dataset")
str(Airport_delay)

print("Lets find the summary of the dataset")
summary(Airport_delay)

attach(Airport_delay)

Airport_delay [Airport_delay == ""] <- NA

Airport_delay %>% group_by(Departure.Airport, Departure.Airport.On.Time.Rating..out.of.10.)%>%summarize()
##Mumbai seems to have the worst rating for departure on time performance

Airport_delay %>% group_by(Arrival.Airport, Arrival.Airport.On.Time.Rating..out.of.10.)%>%summarize()
##Mumbai seems to have the worst rating for Arrival on time performance as well

```



## Cleaning Datasets
```{r}

## Remove the entries from the table where tavg is NA
New_Weather_Bangalore <- Weather_Bangalore[complete.cases(Weather_Bangalore),]
New_Weather_Delhi <- Weather_Delhi[complete.cases(Weather_Delhi),]
New_Weather_Lucknow <- Weather_Lucknow[complete.cases(Weather_Lucknow),]
New_Weather_Mumbai <- Weather_Mumbai[complete.cases(Weather_Mumbai),]
New_Weather_Jodhpur <- Weather_Jodhpur[complete.cases(Weather_Jodhpur),]

## For Bhubhenshwar and Rourkela, we need to first remove the columns snow and tsun which has no valid entries
## We can also remove the wdir, wspd, pressure columns as the other stations are not having them
## And hence having them does not seem to add value for the scope of this analysis
Standard_Weather_Bhubhneshwar <- subset(Weather_Bhubhneshwar, select = -c(snow,wdir,wspd,pres,tsun,wpgt))
New_Weather_Bhubhneshwar <- Standard_Weather_Bhubhneshwar[complete.cases(Standard_Weather_Bhubhneshwar),] 

Standard_Weather_Rourkela <- subset(Weather_Rourkela, select = -c(snow,wdir,wspd,pres,tsun,wpgt))
New_Weather_Rourkela <- Standard_Weather_Rourkela[complete.cases(Standard_Weather_Rourkela),] 


## When it comes to AQI stations, we need only active stations

New_AQ_stations <- AQ_stations %>% filter(Status == "Active")
New_AQ_station_hour <- AQ_station_hour[complete.cases(AQ_station_hour),]
New_AQ_station_day <- AQ_station_day[complete.cases(AQ_station_day),]
New_AQ_city_hour <- AQ_city_hour[complete.cases(AQ_city_hour),]
New_AQ_city_day <- AQ_city_day[complete.cases(AQ_city_day),]

## Clean the Airport Delay data too
New_Airport_delay <- Airport_delay[complete.cases(Airport_delay),]

```
## Detect outliers on Datasets
## Since one of the hypothesis we have here is that extreme weather conditions 
## will affect the flight traffic, we are  really looking for outliers unlike 
## normal cases where we tend to avoid outliers

## Exploratoray Analysis of Bangalore Weather Dataset
```{r}
hist(x=New_Weather_Bangalore$tavg, main = "Bangalore Average Temparature")
## Data outside <20 and >30 are outliers for Bangalore average 

hist(x=New_Weather_Bangalore$tmin, main = "Bangalore Min Temparature")
## Data outside <16 are outliers for Bangalore min 

hist(x=New_Weather_Bangalore$tmax, main = "Bangalore Max  Temparature")
## Data outside >35 are outliers for Bangalore min 

hist(x=New_Weather_Bangalore$prcp, main = "Bangalore Precipitation", breaks = 5)
## Extreme cases are above 50

## So lets make special dataset
Special_Weather_Bangalore <- New_Weather_Bangalore %>% filter((tavg < 20) | (tavg>30) | (tmin < 16) | (tmax > 35) | (prcp > 50))

## And since the precipitation makes the most impact on flights, lets take a look at how precipitation gets impacted by temperatures
ggplot(Special_Weather_Bangalore, aes(x = tmin, 
                     y = tmax, 
                     color = prcp)) +
  geom_point(size = Special_Weather_Bangalore$prcp/75) +
  labs(title = "Impact of temperature on precipitation")

## From the picture looks like the extreme precipitation happens either during when tmin is between 16 to 22
## So, lets put a special filter around that and redraw
Ext_Special_Weather_Bangalore <- Special_Weather_Bangalore %>% filter((tmin > 16) & (tmin < 22))

## And since the precipitation makes the most impact on flights, lets take a look at how precipitation gets impacted by temperatures
ggplot(Ext_Special_Weather_Bangalore, aes(x = tmin, 
                     y = tmax, 
                     color = prcp)) +
  geom_point(size = Ext_Special_Weather_Bangalore$prcp/75) +
  labs(title = "Impact of temperature on precipitation")
```

## Exploratory Analysis of Chennai Weather Dataset
## Exploratory Analysis of Delhi Weather Dataset
```{r}
hist(x=New_Weather_Delhi$tavg, main = "Delhi Average Temparature")
## Data outside <15 and >35 are outliers for Delhi average 

hist(x=New_Weather_Delhi$tmin, main = "Delhi Min Temparature")
## Data outside <16 are outliers for Delhi min 

hist(x=New_Weather_Delhi$tmax, main = "Delhi Max  Temparature")
## Data outside >35 are outliers for Delhi min 

hist(x=New_Weather_Delhi$prcp, main = "Delhi Precipitation", breaks = 5)
## Extreme cases are above 50

## So lets make special dataset
Special_Weather_Delhi <- New_Weather_Delhi %>% filter((tavg < 15) | (tavg>35) | (tmin < 10) | (tmax > 30) | (prcp > 50))

## And since the precipitation makes the most impact on flights, lets take a look at how precipitation gets impacted by temperatures
ggplot(Special_Weather_Delhi, aes(x = tmin, 
                     y = tmax, 
                     color = prcp)) +
  geom_point(size = Special_Weather_Delhi$prcp/50) +
  labs(title = "Impact of temperature on precipitation")

## From the picture looks like the extreme precipitation happens either during when tmin is between 20 to 30
## So, lets put a special filter around that and redraw
Ext_Special_Weather_Delhi <- Special_Weather_Delhi %>% filter((tmin > 20) & (tmin < 30))

## And since the precipitation makes the most impact on flights, lets take a look at how precipitation gets impacted by temperatures
ggplot(Ext_Special_Weather_Delhi, aes(x = tmin, 
                     y = tmax, 
                     color = prcp)) +
  geom_point(size = Ext_Special_Weather_Delhi$prcp/75) +
  labs(title = "Impact of temperature on precipitation")
```


## Exploratory Analysis of Lucknow Weather Dataset
```{r}
hist(x=New_Weather_Lucknow$tavg, main = "Lucknow Average Temparature")
## Data outside <16 and >33 are outliers for Lucknow average 

hist(x=New_Weather_Lucknow$tmin, main = "Lucknow Min Temparature")
## Data outside <15 are outliers for Lucknow min 

hist(x=New_Weather_Lucknow$tmax, main = "Lucknow Max  Temparature")
## Data outside >35 are outliers for Lucknow min 

hist(x=New_Weather_Lucknow$prcp, main = "Lucknow Precipitation", breaks = 5)
## Extreme cases are above 50

## So lets make special dataset
Special_Weather_Lucknow <- New_Weather_Lucknow %>% filter((tavg < 16) | (tavg>33) | (tmin < 15) | (tmax > 30) | (prcp > 50))

## And since the precipitation makes the most impact on flights, lets take a look at how precipitation gets impacted by temperatures
ggplot(Special_Weather_Lucknow, aes(x = tmin, 
                     y = tmax, 
                     color = prcp)) +
  geom_point(size = Special_Weather_Lucknow$prcp/50) +
  labs(title = "Impact of temperature on precipitation")

## From the picture looks like the extreme precipitation happens either during when tmin is between 20 to 30
## So, lets put a special filter around that and redraw
Ext_Special_Weather_Lucknow <- Special_Weather_Lucknow %>% filter((tmin > 20) & (tmin < 30))

## And since the precipitation makes the most impact on flights, lets take a look at how precipitation gets impacted by temperatures
ggplot(Ext_Special_Weather_Lucknow, aes(x = tmin, 
                     y = tmax, 
                     color = prcp)) +
  geom_point(size = Ext_Special_Weather_Lucknow$prcp/75) +
  labs(title = "Impact of temperature on precipitation")
```

## Exploratory Analysis of Mumbai Weather Dataset
```{r}
hist(x=New_Weather_Mumbai$tavg, main = "Mumbai Average Temparature")
## Data outside <25 and >30 are outliers for Mumbai average 

hist(x=New_Weather_Mumbai$tmin, main = "Mumbai Min Temparature")
## Data outside <17 are outliers for Mumbai min 

hist(x=New_Weather_Mumbai$tmax, main = "Mumbai Max  Temparature")
## Data outside >35 are outliers for Mumbai min 

hist(x=New_Weather_Mumbai$prcp, main = "Mumbai Precipitation", breaks = 5)
## Extreme cases are above 50

## So lets make special dataset
Special_Weather_Mumbai <- New_Weather_Mumbai %>% filter((tavg < 25) | (tavg>30) | (tmin < 17) | (tmax > 35) | (prcp > 50))

## And since the precipitation makes the most impact on flights, lets take a look at how precipitation gets impacted by temperatures
ggplot(Special_Weather_Mumbai, aes(x = tmin, 
                     y = tmax, 
                     color = prcp)) +
  geom_point(size = Special_Weather_Mumbai$prcp/50) +
  labs(title = "Impact of temperature on precipitation")

## From the picture looks like the extreme precipitation happens either during when tmin is between 22 to 27
## So, lets put a special filter around that and redraw
Ext_Special_Weather_Mumbai <- Special_Weather_Mumbai %>% filter((tmin > 22) & (tmin < 27))

## And since the precipitation makes the most impact on flights, lets take a look at how precipitation gets impacted by temperatures
ggplot(Ext_Special_Weather_Mumbai, aes(x = tmin, 
                     y = tmax, 
                     color = prcp)) +
  geom_point(size = Ext_Special_Weather_Mumbai$prcp/75) +
  labs(title = "Impact of temperature on precipitation")
```

## Exploratory Analysis of Jodhpur Weather Dataset
```{r}
hist(x=New_Weather_Jodhpur$tavg, main = "Jodhpur Average Temparature")
## Data outside <22 and >28 are outliers for Jodhpur average 

hist(x=New_Weather_Jodhpur$tmin, main = "Jodhpur Min Temparature")
## Data outside <16 are outliers for Jodhpur min 

hist(x=New_Weather_Jodhpur$tmax, main = "Jodhpur Max  Temparature")
## Data outside >33 are outliers for Jodhpur min 

hist(x=New_Weather_Jodhpur$prcp, main = "Jodhpur Precipitation", breaks = 5)
## Extreme cases are above 50

## So lets make special dataset
Special_Weather_Jodhpur <- New_Weather_Jodhpur %>% filter((tavg < 22) | (tavg>28) | (tmin < 16) | (tmax > 33) | (prcp > 50))

## And since the precipitation makes the most impact on flights, lets take a look at how precipitation gets impacted by temperatures
ggplot(Special_Weather_Jodhpur, aes(x = tmin, 
                     y = tmax, 
                     color = prcp)) +
  geom_point(size = Special_Weather_Jodhpur$prcp/50) +
  labs(title = "Impact of temperature on precipitation")

## From the picture looks like the extreme precipitation happens either during when tmin is between 17 to 23
## So, lets put a special filter around that and redraw
Ext_Special_Weather_Jodhpur <- Special_Weather_Jodhpur %>% filter((tmin > 17) & (tmin < 23))

## And since the precipitation makes the most impact on flights, lets take a look at how precipitation gets impacted by temperatures
ggplot(Ext_Special_Weather_Jodhpur, aes(x = tmin, 
                     y = tmax, 
                     color = prcp)) +
  geom_point(size = Ext_Special_Weather_Jodhpur$prcp/75) +
  labs(title = "Impact of temperature on precipitation")
```

## Exploratory Analysis of Bhubhenshwar Weather Dataset
```{r}
hist(x=New_Weather_Bhubhneshwar$tavg, main = "Bhubhenshwar Average Temparature")
## Data outside <24 and >32 are outliers for Bhubhenshwar average 

hist(x=New_Weather_Bhubhneshwar$tmin, main = "Bhubhenshwar Min Temparature")
## Data outside <15 are outliers for Bhubhenshwar min 

hist(x=New_Weather_Bhubhneshwar$tmax, main = "Bhubhenshwar Max  Temparature")
## Data outside >35 are outliers for Bhubhenshwar min 

hist(x=New_Weather_Bhubhneshwar$prcp, main = "Bhubhenshwar Precipitation", breaks = 5)
## Extreme cases are above 50

## So lets make special dataset
Special_Weather_Bhubhenshwar <- New_Weather_Bhubhneshwar %>% filter((tavg < 24) | (tavg>32) | (tmin < 15) | (tmax > 35) | (prcp > 50))

## And since the precipitation makes the most impact on flights, lets take a look at how precipitation gets impacted by temperatures
ggplot(Special_Weather_Bhubhenshwar, aes(x = tmin, 
                     y = tmax, 
                     color = prcp)) +
  geom_point(size = Special_Weather_Bhubhenshwar$prcp/50) +
  labs(title = "Impact of temperature on precipitation")

## From the picture looks like the extreme precipitation happens either during when tmin is between 17 to 27
## So, lets put a special filter around that and redraw
Ext_Special_Weather_Bhubhenshwar <- Special_Weather_Bhubhenshwar %>% filter((tmin > 17) & (tmin < 27))

## And since the precipitation makes the most impact on flights, lets take a look at how precipitation gets impacted by temperatures
ggplot(Ext_Special_Weather_Bhubhenshwar, aes(x = tmin, 
                     y = tmax, 
                     color = prcp)) +
  geom_point(size = Ext_Special_Weather_Bhubhenshwar$prcp/75) +
  labs(title = "Impact of temperature on precipitation")
```

## Exploratory Analysis of Rourkela Weather Dataset
```{r}
hist(x=New_Weather_Rourkela$tavg, main = "Rourkela Average Temparature")
## Data outside <20 and >32 are outliers for Rourkela average 

hist(x=New_Weather_Rourkela$tmin, main = "Rourkela Min Temparature")
## Data outside <15 are outliers for Rourkela min 

hist(x=New_Weather_Rourkela$tmax, main = "Rourkela Max  Temparature")
## Data outside >35 are outliers for Rourkela min 

hist(x=New_Weather_Rourkela$prcp, main = "Rourkela Precipitation", breaks = 5)
## Extreme cases are above 40

## So lets make special dataset
Special_Weather_Rourkela <- New_Weather_Rourkela %>% filter((tavg < 20) | (tavg>32) | (tmin < 15) | (tmax > 30) | (prcp > 40))

## And since the precipitation makes the most impact on flights, lets take a look at how precipitation gets impacted by temperatures
ggplot(Special_Weather_Rourkela, aes(x = tmin, 
                     y = tmax, 
                     color = prcp)) +
  geom_point(size = Special_Weather_Rourkela$prcp/50) +
  labs(title = "Impact of temperature on precipitation")

## From the picture looks like the extreme precipitation happens either during when tmin is between 22 to 27
## So, lets put a special filter around that and redraw
Ext_Special_Weather_Rourkela <- Special_Weather_Rourkela %>% filter((tmin > 22) & (tmin < 27))

## And since the precipitation makes the most impact on flights, lets take a look at how precipitation gets impacted by temperatures
ggplot(Ext_Special_Weather_Rourkela, aes(x = tmin, 
                     y = tmax, 
                     color = prcp)) +
  geom_point(size = Ext_Special_Weather_Rourkela$prcp/75) +
  labs(title = "Impact of temperature on precipitation")

```



## Exploratory Analysis of AQI data station wise
```{r}

head(New_AQ_station_hour)
# Lets see the performance of the AQI over years
AQ_station_Day_Sep <- New_AQ_station_hour %>% separate(Datetime, c('Date', 'Time'), sep =" ") %>% separate(Time, c('Hr', 'Min', 'Sec'), sep=":") %>% mutate(Hour = as.numeric(Hr))

AQ_station_Day_Duration <- AQ_station_Day_Sep %>% mutate(Duration=cut(Hour, breaks=c(-1, 6, 18, 24),labels=c("Early_Morning","Day","Night")))

AQI_Over_Years <- AQ_station_Day_Duration%>% group_by(YEAR = year(ymd(Date)), AQI_Bucket) %>% summarize(Mean_AQI = mean(AQI))
ggplot(AQI_Over_Years, aes(x = YEAR, y = Mean_AQI, color=AQI_Bucket))+  geom_line()

## It appears that 'Severe' and 'Poor' cases didn't exist much  until 2017 from which these
## two gained at the behest of 'Good' AQI cases

# Lets see the performance of the AQI over a day in every year

AQI_Over_Time <- AQ_station_Day_Duration %>% group_by(YEAR = year(ymd(Date)), Duration) %>% summarize(Mean_AQI = mean(AQI))
ggplot(AQI_Over_Time, aes(x = YEAR, y = Mean_AQI, color = Duration))+  geom_point(size = AQI_Over_Time$Duration)

## We can see that the year 2017 had witnessed the worst air quality index but much of that
## was during the day time. Things slowed down in the years later but in them, 
## but the pattern changed by having night time pollution as the worst.
## In all cases, early morning pollution was the lowest.


# Lets see the performance of the AQI monthwise
AQI_monthwise <- AQ_station_Day_Duration %>% group_by(Month = month(ymd(Date), label = TRUE), Duration) %>% summarize(Mean_AQI = mean(AQI))
ggplot(AQI_monthwise, aes(x = Month, y = Mean_AQI, color = Duration))+  geom_point(size = AQI_monthwise$Duration)

## We can see that the colder months - i.e., from Oct to Feb, the AQI is the worst, its bad during summer but it appears the best in monsoon season.

AQI_Over_month <- AQ_station_Day_Duration %>% group_by(YEAR = year(ymd(Date)), Month = month(ymd(Date), label = TRUE), Duration) %>% summarize(Mean_AQI = mean(AQI))

# Lets see if how this works out yearwise and monthwise
ggplot(AQI_Over_month, aes(x = Month, y = Mean_AQI, color = Duration))+  geom_point(size = AQI_Over_month$Duration) + facet_wrap(~YEAR)

## We can see the same trend every year - i.e., the colder months has the worst AQI while the monsoon has the best AQI while summer/spring time having the intermediate values

## Now lets report this city wise - probably for the Month wise combination
AQI_Stationwise <- AQ_station_Day_Duration %>% group_by(Station = StationId, YEAR = year(ymd(Date)), Month = month(ymd(Date), label = TRUE), Duration) %>% summarize(Mean_AQI = mean(AQI))
ggplot(AQI_Stationwise, aes(x = Month, y = Mean_AQI, color = Station))+  geom_point(shape = AQI_Stationwise$Duration)

## Across stations, the trend seems to be the same - i.e., worst during winter, intermediate during spring/summer, best during monsoon.

## Now out of the 19 stations, we are very interested on just interested on Delhi for which we are going to do air traffic impact analysis - so lets filter them and zoom into their performance alone
AQI_Delhi_Station <- AQ_station_Day_Duration %>% filter( (StationId == "DL001") | (StationId == "DL019")) %>% group_by(Station = StationId, YEAR = year(ymd(Date)), Month = month(ymd(Date), label = TRUE), Duration) %>% summarize(Mean_AQI = mean(AQI))
ggplot(AQI_Delhi_Station, aes(x = Month, y = Mean_AQI, color = Duration))+  geom_point(size = AQI_Delhi_Station$Duration) + facet_wrap(~YEAR)

# Lets see how AQ day data is different from station hour wise data
New_AQ_station_day_Years <- New_AQ_station_day%>% group_by(YEAR = year(ymd(Date)), AQI_Bucket) %>% summarize(Mean_AQI = mean(AQI))

head(AQ_station_Day_Duration)
## There seems to be nothing new that we can derive out of the station day wise that we can't derive out of 
## station hour wise data. so no further analysis needed over here


```

## Exploratory Analysis of AQI data city wise
```{r}
## Lets look at City wise hourly AQI data
head(New_AQ_city_hour)

# Lets see the performance of the AQI over years
AQ_city_Day_Sep <- New_AQ_city_hour %>% separate(Datetime, c('Date', 'Time'), sep =" ") %>% separate(Time, c('Hr', 'Min', 'Sec'), sep=":") %>% mutate(Hour = as.numeric(Hr))

AQ_city_Day_Duration <- AQ_city_Day_Sep %>% mutate(Duration=cut(Hour, breaks=c(-1, 6, 18, 24),labels=c("Early_Morning","Day","Night")))

## Now get it grouped by Year and plot year wise performance
AQI_City_Over_Years <- AQ_city_Day_Duration%>% group_by(YEAR = year(ymd(Date)), AQI_Bucket) %>% summarize(Mean_AQI = mean(AQI))

ggplot(AQI_City_Over_Years, aes(x = YEAR, y = Mean_AQI, color=AQI_Bucket))+  geom_line()

AQI_City_Over_Time <- AQ_city_Day_Duration %>% group_by(YEAR = year(ymd(Date)), Duration) %>% summarize(Mean_AQI = mean(AQI))
ggplot(AQI_City_Over_Time, aes(x = YEAR, y = Mean_AQI, color = Duration))+  geom_point(size = AQI_City_Over_Time$Duration)

## It appears 2015 had peak values of AQIs, which dropped to very low in 2016, gained to half the levels back in 2017 and then gradually reducing
## We can see that 2015-2017 worst was during day time but from 2018, there were worse night times - may be something to do with dropped levels of AQIs as well
## In all cases, early morning pollution seems to be the lowest.


# Lets see the performance of the AQI month wise
AQI_City_monthwise <- AQ_city_Day_Duration %>% group_by(Month = month(ymd(Date), label = TRUE), Duration) %>% summarize(Mean_AQI = mean(AQI))
ggplot(AQI_City_monthwise, aes(x = Month, y = Mean_AQI, color = Duration))+  geom_point(size = AQI_City_monthwise$Duration)
## We can see that the winter months - i.e., from Oct to Feb, the AQI is the worst, its bad during summer but it appears the best in monsoon season. The difference between stationwise data is that, here Nov seems to be the worst month while in ther other dataset, Dec held the worst...

AQI_City_Over_month <- AQ_city_Day_Duration %>% group_by(YEAR = year(ymd(Date)), Month = month(ymd(Date), label = TRUE), Duration) %>% summarize(Mean_AQI = mean(AQI))
AQI_City_Over_month

# Lets see if how this works out yearwise and monthwise
ggplot(AQI_City_Over_month, aes(x = Month, y = Mean_AQI, color = Duration))+  geom_point(size = AQI_City_Over_month$Duration) + facet_wrap(~YEAR)

## We can see the same trend every year - i.e., the winter months has the worst AQI while the monsoon has the best AQI while summer/spring time having the intermediate values

## Now lets report this city wise - probably for the Month wise combination
AQI_Citywise <- AQ_city_Day_Sep %>% group_by(City, YEAR = year(ymd(Date)), Month = month(ymd(Date), label = TRUE)) %>% summarize(Mean_AQI = mean(AQI))
ggplot(AQI_Citywise, aes(x = Month, y = Mean_AQI))+  geom_point(aes(color=City))

## Across stations, the trend seems to be the same - i.e., worst during winter, intermediate during spring/summer, best during monsoon.

## Now out of all the cities, we are very interested on Delhi for which we are going to do air traffic impact analysis - so lets filter them and zoom into their performance alone
AQI_Delhi_City <- AQ_city_Day_Sep %>% filter( City == "Delhi") %>% group_by(City, YEAR = year(ymd(Date)), Month = month(ymd(Date), label = TRUE)) %>% summarize(Mean_AQI = mean(AQI))
ggplot(AQI_Delhi_City, aes(x = Month, y = Mean_AQI))+  geom_point() + facet_wrap(~YEAR)


New_AQ_city_day_Years <- New_AQ_city_day%>% group_by(City, YEAR = year(ymd(Date)), Month = month(ymd(Date), label = TRUE)) %>% summarize(Mean_AQI = mean(AQI))
head(New_AQ_city_day_Years)
ggplot(New_AQ_city_day_Years, aes(x = Month, y = Mean_AQI))+  geom_point(aes(color=City))

## There seems to be small difference when comparing hour wise data to day wise data, but not significant enough. So we will use mainly  the hour wise datafor citiwise analysis.


```
## Model for the prediction of AQI index
```{r}
## We would like to understand which of the parameters are really affecting AQI value.
## Based on the analysis above we will stick to using the Cleaned Station hour wise datasets.

New_AQ_station_hour_sep <- New_AQ_station_hour %>% separate(Datetime, c('Date', 'Time'), sep =" ") %>% separate(Time, c('Hr', 'Min', 'Sec'), sep=":") %>% mutate(Hour = as.numeric(Hr), Month = month(ymd(Date)))

## Now lets focus on the months where we have the most troubles with AQI - Oct to Feb
New_AQ_station_hour_sep_BM <- New_AQ_station_hour_sep %>% filter ((Month == 1) | (Month == 2) | (Month == 10) | (Month == 11) | (Month == 12))

AQI_O3_model <- lm(AQI~O3, data = New_AQ_station_hour_sep)
fmodel(AQI_O3_model)
## OK vow, looks like AQI has direct relationship with the O3 content

AQI_O3_model_BM <- lm(AQI~O3, data = New_AQ_station_hour_sep_BM)
fmodel(AQI_O3_model_BM)

## In bad months looks like O3 and AQI are inversely proportional

## Lets try with PM2.5
AQI_PM_2_5_model <- lm(AQI~PM2.5, data = New_AQ_station_hour_sep)
fmodel(AQI_PM_2_5_model)
## OK even here there is an impact - actually much more
AQI_PM_2_5_model_BM <- lm(AQI~PM2.5, data = New_AQ_station_hour_sep_BM)
fmodel(AQI_PM_2_5_model_BM)
## PM2.5 impact seems to be much higher over the winter months

##Lets try others
AQI_PM_10_model <- lm(AQI~PM10, data = New_AQ_station_hour_sep)
fmodel(AQI_PM_10_model)
AQI_PM_10_model_BM <- lm(AQI~PM10, data = New_AQ_station_hour_sep_BM)
fmodel(AQI_PM_10_model_BM)

## No significant impact change in winter months for PM10

AQI_NO_model <- lm(AQI~NO, data = New_AQ_station_hour_sep)
fmodel(AQI_NO_model)
AQI_NO_model_BM <- lm(AQI~NO, data = New_AQ_station_hour_sep_BM)
fmodel(AQI_NO_model_BM)
## Slight reduction in winter months for NO

AQI_NO2_model <- lm(AQI~NO2, data = New_AQ_station_hour_sep)
fmodel(AQI_NO2_model)
AQI_NO2_model_BM <- lm(AQI~NO2, data = New_AQ_station_hour_sep_BM)
fmodel(AQI_NO2_model_BM)
## No significant impact change in winter months for NO2

AQI_NOx_model <- lm(AQI~NOx, data = New_AQ_station_hour_sep)
fmodel(AQI_NOx_model)
AQI_NOx_model_BM <- lm(AQI~NOx, data = New_AQ_station_hour_sep_BM)
fmodel(AQI_NOx_model_BM)
## Slight reduction in winter months for NOx

AQI_NH3_model <- lm(AQI~NH3, data = New_AQ_station_hour_sep)
fmodel(AQI_NH3_model)
AQI_NH3_model_BM <- lm(AQI~NH3, data = New_AQ_station_hour_sep_BM)
fmodel(AQI_NH3_model_BM)
## PM2.5 impact seems to be much higher (50% more) over the winter months

AQI_CO_model <- lm(AQI~CO, data = New_AQ_station_hour_sep)
fmodel(AQI_CO_model)
AQI_CO_model_BM <- lm(AQI~CO, data = New_AQ_station_hour_sep_BM)
fmodel(AQI_CO_model_BM)
## No significant impact change in winter months for CO

AQI_SO2_model <- lm(AQI~SO2, data = New_AQ_station_hour_sep)
fmodel(AQI_SO2_model)
AQI_SO2_model_BM <- lm(AQI~SO2, data = New_AQ_station_hour_sep_BM)
fmodel(AQI_SO2_model_BM)
## Slight reduction in winter months for SO2

AQI_Benzene_model <- lm(AQI~Benzene, data = New_AQ_station_hour_sep)
fmodel(AQI_Benzene_model)
AQI_Benzene_model <- lm(AQI~Benzene, data = New_AQ_station_hour_sep_BM)
fmodel(AQI_Benzene_model)
## Slight reduction in winter months for Benzene

AQI_Toluene_model <- lm(AQI~Toluene, data = New_AQ_station_hour_sep)
fmodel(AQI_Toluene_model)
AQI_Toluene_model_BM <- lm(AQI~Toluene, data = New_AQ_station_hour_sep_BM)
fmodel(AQI_Toluene_model_BM)
## Slight reduction in winter months for Toulene

AQI_Xylene_model <- lm(AQI~Xylene, data = New_AQ_station_hour_sep)
fmodel(AQI_Xylene_model)
AQI_Xylene_model_BM <- lm(AQI~Xylene, data = New_AQ_station_hour_sep_BM)
fmodel(AQI_Xylene_model_BM)
## No significant impact change in winter months for Xylene

## Among these, the highest impact seems to be from CO. Bringing in
## O3 due to their peculiar reversal in Winter months
AQI_High_Impact_model <- lm(AQI~O3+CO, data = New_AQ_station_hour_sep)
fmodel(AQI_High_Impact_model)

AQI_High_Impact_model_BM <- lm(AQI~O3+CO, data = New_AQ_station_hour_sep_BM)
fmodel(AQI_High_Impact_model_BM)

evaluate_model(AQI_High_Impact_model)
evaluate_model(AQI_High_Impact_model_BM)

## Defintely bad months brings in a lot of diffeence into the data set.
## So lets consider even months as one explanatory variables
New_AQ_station_hour_sep
AQI_High_Impact_model_Month <- lm(AQI~O3+CO+month(ymd(Date)), data = New_AQ_station_hour_sep)
fmodel(AQI_High_Impact_model_Month)
evaluate_model(AQI_High_Impact_model_Month)


## Having month as part of the model really makes a difference to the evaluation.

## Now lets train the model and see if we can predict the values of AQI
#make this split reproducible
set.seed(1)

#Use 70% of dataset as training set and remaining 30% as testing set
AQI_sample_set <- sample(c(TRUE, FALSE), nrow(New_AQ_station_hour_sep), replace=TRUE, prob=c(0.7,0.3))
AQI_train_dataset  <- New_AQ_station_hour_sep[AQI_sample_set, ]
AQI_test_dataset   <- New_AQ_station_hour_sep[!AQI_sample_set, ]

AQI_Eval_model = lm(AQI~O3+CO+month(ymd(Date)), data = AQI_train_dataset)
summary(AQI_Eval_model)

Predicted_AQI_Values <- predict(AQI_Eval_model, AQI_test_dataset)

AQI_test_dataset["Predicted_AQI"] <- Predicted_AQI_Values

Summary_AQI_Model_Performace <- AQI_test_dataset %>% group_by(YEAR = year(ymd(Date)), Month) %>% summarise(AQI, Predicted_AQI)
Summary_AQI_Model_Performace

ggplot(Summary_AQI_Model_Performace, aes(x = Month)) +
        geom_point(aes(y = AQI, color = 'AQI')) +
        geom_point(aes(y = Predicted_AQI, color = 'Predictede_AQI')) +
         scale_x_continuous(breaks=seq(1, 12, by = 1))+
  labs(title = "AQI Model Performance")  + facet_wrap(~YEAR)


## We can see that there are a good amount overlaps between the AQI prediction vs actual data though
## there is still a very large scope of improvement of the model - esp when dealing with outliers.
## But so far we have sufficient proof available that AQI is heavily influenced by 
## month of the year and quantities of O3 and CO.

```

## Form a cohesive Delhi dataset
```{r}
## We have seen how components of air impacted AQI
## Time to see the impact of weather on AQI by merging the station day wise data with the weather data
## Please note we are not picking up station hour wise data because the weather data we have is only day wise data

## Out of the cities for which weather has been provided, the only city that overlaps with the AQI data is Delhi
## And ofcourse we are trying to find the impact of AQI on Airtraffic in Delhi, so lets bring in that too
## So lets merge these three datasets only for Delhi

Delhi_AQI_data_temp <- New_AQ_station_day %>% filter ((StationId == "DL001") | (StationId == "DL019"))%>% mutate(Date_1 = ymd(as.Date(Date)))
Delhi_AQI_data <- Delhi_AQI_data_temp[, -2] %>% rename("Date" = "Date_1")

New_Weather_Delhi_day <- New_Weather_Delhi %>% mutate(Date = dmy(time))

Delhi_Airport_Delay_date <- New_Airport_delay %>% filter (Departure.Airport == "DEL") %>% mutate(Date_1 = dmy(Date))
Delhi_Airport_Delay_rename <- Delhi_Airport_Delay_date[, -1] %>% rename("Date" = "Date_1")
Delhi_Airport_Delay_date_sorted <- Delhi_Airport_Delay_rename[order(Delhi_Airport_Delay_rename$Date),]

## The range of weather data is from 01/01/1990 to 25/07/2022
## The range of airport delay data is from 28/01/18 to 27/1/2020

## So the overlapping range is from 1/11/2018 to 26/1/2020

Delhi_Airport_Delay_range <- Delhi_Airport_Delay_date_sorted %>% filter (between(Date, as.Date('2018-01-25'), as.Date('2020-01-27')))
#Delhi_Airport_Delay <- Delhi_Airport_Delay_dates %>% filter ((Date >'25-01-18') & (Date < '29-01-20'))  #1925
New_Weather_Delhi_day_range <- New_Weather_Delhi_day %>% filter (between(Date, as.Date('2018-01-25'), as.Date('2020-01-27')))

Delhi_AQI_data_range <- Delhi_AQI_data %>% filter (between(Date, as.Date('2018-01-25'), as.Date('2020-01-27')))

##Delhi_Airport_Delay data has multiple entries for a day as it is cutting across many airliners operating on an airport. But we are interested in average delay per day and not really on the airliner related information. So, lets clean the data a bit there.

convert_min <- function(x)
{
  if(x < 0)
  {
    time_mins = 0
  }
  else
  {
    time_d <- hms(x)
    time_mins <- hour(time_d)*60 + minute(time_d)
  }
}

Delhi_Airport_Delay_in_min <- Delhi_Airport_Delay_range %>% mutate (Departure_Delay_min = unlist(lapply(Departure.Delay, convert_min)), Arrival_Delay_min = unlist(lapply(Arrival.Time.Delay, convert_min)))
Delhi_Airport_Delay_datewise <- Delhi_Airport_Delay_in_min  %>% group_by(Date) %>% summarize(Daily_Delay = sum(Departure_Delay_min + Arrival_Delay_min))

Delhi_AQI_weather_data_merge_temp <- merge(New_Weather_Delhi_day, Delhi_AQI_data)
Delhi_AQI_weather_data_merge_temp_1 <- Delhi_AQI_weather_data_merge_temp[,-2]
Delhi_cohesive_dataset <- merge(Delhi_AQI_weather_data_merge_temp_1, Delhi_Airport_Delay_datewise)%>% mutate(Month = month(ymd(Date)))

head(Delhi_cohesive_dataset)
#Looks like the merger is successful with no NA
#Lets summarize full dataset
summary(Delhi_cohesive_dataset)
```

## Analyse cohesive dataset a bit to understand how delay and other parameters plot each other
```{r}
names(Delhi_cohesive_dataset)
ggplot(Delhi_cohesive_dataset, aes(x = AQI, y = Daily_Delay, color = AQI_Bucket, size = prcp)) +
  geom_point() +
  labs(title = "Impact of AQI and prcp")

##As per the plot, Good AQI too gets observed for some delay cases but they are far and few... and does not seems to have caused high amount of delays 
## There area huge amount of delays caused for satisfactory AQI cases but most of the delays could be associated 
## with pretty high precipitation
## There are good amount of delays associate with moderate cases too and they do have caused significant delays when combined with high precipitations
## Delay instances reduces for Poor AQI cases but there is a slight increase in the values of delays
## For very poor cases, impact gets high when combined with precipitation
## Severe cases are high impact ones but looks like not affected with precipitation


## Now lets view this purely from the weather perspective
names(Delhi_cohesive_dataset)
ggplot(Delhi_cohesive_dataset, aes(x = tavg, y = Daily_Delay, color = tmin, size = prcp)) +
  geom_point() +
  labs(title = "Impact of temp and prcp")

## Its clear that bigger precipitation brings in more instances of delays
## But its also interesting to find that higher tavg, higher precipitation and higher tmin bring 
# in a lot of delays - though size of precipitation does not always result in costly delays

## Ok Lets also analyse if the components O3, PM2.5 and CO has impacts on delays

ggplot(Delhi_cohesive_dataset, aes(x = O3, y = Daily_Delay, size = O3)) +
  geom_point() +
  labs(title = "Impact of O3")

## Looks like more O3 directly relates to higher delays

ggplot(Delhi_cohesive_dataset, aes(x = PM2.5, y = Daily_Delay, size = PM2.5)) +
  geom_point() +
  labs(title = "Impact of PM2.5")

## Looks like more PM2.5 might not have too much impact...

ggplot(Delhi_cohesive_dataset, aes(x = CO, y = Daily_Delay, size = CO)) +
  geom_point() +
  labs(title = "Impact of CO")

## Looks like size of CO has some correlation but may not be linear...

ggplot(Delhi_cohesive_dataset, aes(x = PM10, y = Daily_Delay, size = PM10)) +
  geom_point() +
  labs(title = "Impact of PM10")

## Looks like more PM2.5 might not have too much impact...

ggplot(Delhi_cohesive_dataset, aes(x = prcp, y = Daily_Delay, size = prcp)) +
  geom_point() +
  labs(title = "Impact of rain")

## Looks like amount of rain has direct impact on delays...


ggplot(Delhi_cohesive_dataset, aes(x = tavg, y = Daily_Delay, size = tavg)) +
  geom_point() +
  labs(title = "Impact of Average Temp")

## Looks like a lot of low intensity delays on higher average temprature...

ggplot(Delhi_cohesive_dataset, aes(x = tmin, y = Daily_Delay, size = tmin)) +
  geom_point() +
  labs(title = "Impact of Tmin")
## Looks like a lot of low intensity delays on higher average temprature...

ggplot(Delhi_cohesive_dataset, aes(x = AQI, y = Daily_Delay, size = AQI, color=AQI_Bucket)) +
  geom_point() +
  labs(title = "Impact of AQI")


## Looks like a lot of low intensity delays on higher Tmin...

## Lets see if the months itself has any impact on the delay

ggplot(Delhi_cohesive_dataset, aes(x = Month, y = Daily_Delay, size = Daily_Delay)) +
  geom_point()+ scale_x_continuous(breaks=seq(1, 12, by = 1))+
  labs(title = "Impact of Month")

## Looks like there is high frequency of delays during monsoon and heavy delay during peak winter season


## Ok based on this, lets pick these elements to find the right model on impacts the delays of Delhi airtraffic:
## Precipitation, AQI, tmin, O3 and CO

## Lets see how the elements individually have linear regression relationship with the traffic delay


## Ok lets build the base model here
Delhi_Traffic_Delay_Model_AQI = lm(Daily_Delay ~ AQI, data = Delhi_cohesive_dataset)
fmodel(Delhi_Traffic_Delay_Model_AQI)

Delhi_Traffic_Delay_Model_tavg = lm(Daily_Delay ~ AQI+tavg, data = Delhi_cohesive_dataset)
fmodel(Delhi_Traffic_Delay_Model_tavg)

Delhi_Traffic_Delay_Model_prcp = lm(Daily_Delay ~ AQI+prcp, data = Delhi_cohesive_dataset)
fmodel(Delhi_Traffic_Delay_Model_prcp)

Delhi_Traffic_Delay_Model_O3 = lm(Daily_Delay ~ AQI+O3, data = Delhi_cohesive_dataset)
fmodel(Delhi_Traffic_Delay_Model_O3)

Delhi_Traffic_Delay_Model_CO = lm(Daily_Delay ~ AQI+CO, data = Delhi_cohesive_dataset)
fmodel(Delhi_Traffic_Delay_Model_CO)

Delhi_Traffic_Delay_Model_Month = lm(Daily_Delay ~ AQI+Month, data = Delhi_cohesive_dataset)
fmodel(Delhi_Traffic_Delay_Model_Month)


evaluate_model(Delhi_Traffic_Delay_Model_AQI)
evaluate_model(Delhi_Traffic_Delay_Model_tavg, tavg = 35)
evaluate_model(Delhi_Traffic_Delay_Model_prcp, prcp = 150)
evaluate_model(Delhi_Traffic_Delay_Model_O3, O3 = 50)
evaluate_model(Delhi_Traffic_Delay_Model_CO, CO = 1)
evaluate_model(Delhi_Traffic_Delay_Model_Month, Month = 12)

diff_1 <- 75.90179 - 65.89103
diff_1
diff_2 <- 74.13083 - 65.28536	
diff_2
diff_3 <- 221.3815 - 207.8647
diff_3
diff_4 <- 86.52229 - 75.87323
diff_4
diff_5 <- 76.02051 - 66.60121
diff_5
diff_6 <- 100.87275	- 84.65495
diff_6
# Comparing the model evalution based on above, we can see that prcp, month and O3 has good impact
# on the delay

## To evaluate the base model, split the data into test and train datasets

#make this split reproducible
set.seed(1)

#Use 70% of dataset as training set and remaining 30% as testing set
sample_set <- sample(c(TRUE, FALSE), nrow(Delhi_cohesive_dataset), replace=TRUE, prob=c(0.7,0.3))
train_dataset  <- Delhi_cohesive_dataset[sample_set, ]
test_dataset   <- Delhi_cohesive_dataset[!sample_set, ]

# the base model with just AQI and tavg
Base_Model_Delay = lm(Daily_Delay ~ AQI+prcp, data = train_dataset)
# the Augmented model with precipitation as well
Aug_Model_Delay = lm(Daily_Delay ~ AQI+prcp+Month, data = train_dataset)
# Run cross validation trials on the two models
trials <- cv_pred_error(Base_Model_Delay, Aug_Model_Delay)


# Compare the two sets of cross-validated errors
t.test(mse ~ model, data = trials)

# t-statistic is -2.8426. degrees of freedom, df is 5.7322 are the degrees of freedom. These are used with a t-distribution to derive p-value of 0.03099

# p-value = 0.03099 - i.e., Given that there is no actual/true difference in means, if we repeat the experiment over and over again, 3.1% of the time we would see the type of difference in means as in your samples, or a more extreme difference in means. Since p value is significantly lower than 0.05, the differences are significant.
# So we can reject the null hypothesis (H0) of no difference between the (true) averages of the two groups
#alternative hypothesis: true difference in means is not equal to 0
#95 percent confidence interval:
# -185.63143 1-12.83777
#If assume H0 is false, the true mean may lie in the interval [3091.307 3190.542].
# So we will chose the augmented model - i.e., Daily_Delay ~ AQI+prcp+Mont

```

## Model for predicting Delhi air traffic delays
```{r}
## For our model to predict the air traffic delays:
## Response Variable is Daily_Delay
## Explanatory Variables are Precipitation (prcp), AQI and Month

## We are choosing a linear regression model here because this is about predicting the numerical values
## and does not belong to classification modelling
Delhi_Traffic_Delay_Model = lm(Daily_Delay ~ AQI+prcp+Month, data = train_dataset)
summary(Delhi_Traffic_Delay_Model)

Predicted_Traffic_Delay <- predict(Delhi_Traffic_Delay_Model, test_dataset)
Predicted_Traffic_Delay

test_dataset["Predicted_Delay"] <- Predicted_Traffic_Delay

Summary_Model_Performace <- test_dataset %>% group_by(YEAR = year(ymd(Date)), Month) %>% summarise(Daily_Delay, Predicted_Delay)
Summary_Model_Performace

ggplot(Summary_Model_Performace, aes(x = Month)) +
        geom_point(aes(y = Daily_Delay, color = 'Daily_Delay')) +
        geom_point(aes(y = Predicted_Delay, color = 'Predictede_Delay')) +
         scale_x_continuous(breaks=seq(1, 12, by = 1))+
  labs(title = "Model Performance")  + facet_wrap(~YEAR)

# As we can see, the model is performing a bit OK for some months except for certain extreme
# cases of delays. So, the model needs further fine tuning or dataset needs to be reanalyzed.
```